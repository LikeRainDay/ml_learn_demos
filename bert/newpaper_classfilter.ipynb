{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "安装所需要的依赖\n",
    "\n",
    "```\n",
    "pip install transformers tqdm boto3 requests regex -q\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 21128\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_version = 'bert-base-chinese'\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们通过随机抽取查看一下词典中的数据对应"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token               index          \n",
      "-------------------------\n",
      "cnn                  9206\n",
      "##lia               10336\n",
      "##歪                 16696\n",
      "##蟋                 19151\n",
      "##椹                 16554\n",
      "闹                    7317\n",
      "専                    2197\n",
      "##•                 13499\n",
      "寞                    2174\n",
      "##濛                 17145\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids):\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. 准备原始文本数据\n",
    "2. 将原始文本转换为bert相关输入格式\n",
    "3. 在bert之上加入新layer成下游任务模型\n",
    "4. 训练该下游任务模型\n",
    "5. 对新样本做推测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始样本数量： 320543\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 对数据进行清洗\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "empty_title = (df_train['title2_zh'].isnull() \\\n",
    "              | df_train['title1_zh'].isnull() \\\n",
    "              | (df_train['title2_zh'] == '') \\\n",
    "              | (df_train['title2_zh'] == '0'))\n",
    "df_train = df_train[~empty_title]\n",
    "\n",
    "print(\"原始样本数量：\", len(df_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本数量： 2657\n"
     ]
    },
    {
     "data": {
      "text/plain": "                          text_a                          text_b      label\n0       苏有朋要结婚了，但网友觉得他还是和林心如比较合适  好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！  unrelated\n1  爆料李小璐要成前妻了贾乃亮模仿王宝强一步到位、快刀斩乱麻！  李小璐要变前妻了？贾乃亮可能效仿王宝强当机立断，快刀斩乱麻！     agreed\n2  为彩礼，母亲把女儿嫁给陌生男子，十年后再见面，母亲湿了眼眶  阿姨，不要彩礼是觉得你家穷，给你台阶下，不要以为我嫁不出去！  unrelated\n3         猪油是个宝，一勺猪油等于十副药，先备起来再说  传承千百的猪油为何变得人人唯恐避之不及？揭开猪油的四大谣言！  unrelated\n4                  剖析：香椿，为什么会致癌？            香椿含亚硝酸盐多吃会致癌？测完发现是谣言  disagreed",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_a</th>\n      <th>text_b</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>苏有朋要结婚了，但网友觉得他还是和林心如比较合适</td>\n      <td>好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>爆料李小璐要成前妻了贾乃亮模仿王宝强一步到位、快刀斩乱麻！</td>\n      <td>李小璐要变前妻了？贾乃亮可能效仿王宝强当机立断，快刀斩乱麻！</td>\n      <td>agreed</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>为彩礼，母亲把女儿嫁给陌生男子，十年后再见面，母亲湿了眼眶</td>\n      <td>阿姨，不要彩礼是觉得你家穷，给你台阶下，不要以为我嫁不出去！</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>猪油是个宝，一勺猪油等于十副药，先备起来再说</td>\n      <td>传承千百的猪油为何变得人人唯恐避之不及？揭开猪油的四大谣言！</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>剖析：香椿，为什么会致癌？</td>\n      <td>香椿含亚硝酸盐多吃会致癌？测完发现是谣言</td>\n      <td>disagreed</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 剔除过长的样本数据避免 bert 无法将整个输入序列放入记忆力不强的 GPU\n",
    "MAX_LENGTH = 30\n",
    "df_train = df_train[~(df_train.title1_zh.apply(lambda x: len(x)) > MAX_LENGTH)]\n",
    "df_train = df_train[~(df_train.title2_zh.apply(lambda x: len(x)) > MAX_LENGTH)]\n",
    "\n",
    "# 只采用1%训练数据\n",
    "SAMPLE_FRAC = 0.01\n",
    "df_train = df_train.sample(frac=SAMPLE_FRAC, random_state=9527)\n",
    "\n",
    "# 去除不必要的单位并重新民命标题内容\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.loc[:, ['title1_zh', 'title2_zh', 'label']]\n",
    "df_train.columns = ['text_a', 'text_b', 'label']\n",
    "\n",
    "# idempotence, 将结果另存为 tsv 供 PyTorch 使用\n",
    "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"训练样本数量：\", len(df_train))\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于样本数据中的unrelated 的数量为68% ,因此我们用的bert训练出的最少要超过68% 的baseline才可以："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "unrelated    0.679338\nagreed       0.294317\ndisagreed    0.026346\nName: label, dtype: float64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts() / len(df_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对测试数据进行处理，满足格式要求"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测样本数据： 80126\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            text_a                       text_b      Id\n0  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？  321187\n1              萨达姆被捕后告诫美国的一句话，发人深思    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国  321190\n2    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗          萨达姆被捕后告诫美国的一句话，发人深思  321189\n3              萨达姆被捕后告诫美国的一句话，发人深思  被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！  321193\n4              萨达姆被捕后告诫美国的一句话，发人深思         中国川贝枇杷膏在美国受到热捧？纯属谣言！  321191",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_a</th>\n      <th>text_b</th>\n      <th>Id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n      <td>321187</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n      <td>321190</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n      <td>321189</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n      <td>321193</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n      <td>321191</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_test = df_test.loc[:, [\"title1_zh\", \"title2_zh\", \"id\"]]\n",
    "df_test.columns = [\"text_a\", \"text_b\", \"Id\"]\n",
    "df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"预测样本数据：\", len(df_test))\n",
    "df_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据样本树 / 训练数据样本书 = 30.2 倍\n"
     ]
    }
   ],
   "source": [
    "ratio = len(df_test) / len(df_train)\n",
    "print(\"测试数据样本树 / 训练数据样本书 = {:.1f} 倍\".format(ratio))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将数据内容转化为 dataset 相容的格式内容"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "作为一个可以用来读取训练 / 测试集的 Dataset, 这是你需要彻底了解的部分，\n",
    "此 Dataset 每次將 tsv 里的一组成对句子转化成 BERT 相容的格式，并回传3哥 tensors：\n",
    "- tokens_tensor：两个句子合并后的索引序列，包含 [CLS] 與 [SEP]\n",
    "- segments_tensor：可以用来识别两个句子的界限 binary tensor\n",
    "- label_tensor：将分类目标转化为类别索引 tensor, 如果是测试集则回传 None\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pysnooper\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    # 读取之前初始化后的 tsv 并初始化一部分参数\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        # 一般训练会需要 dev set\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        self.mode = mode\n",
    "        # 大数据你会需要用 iterator = True\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # 定义回传一些训练 / 测试数据函数\n",
    "    @pysnooper.snoop()\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a, test_b = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "\n",
    "        # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        print(tokens_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "\n",
    "        # 第二个句子的 BERT tokens\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "\n",
    "        # 将整个token 序列转化成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "\n",
    "        # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, dtype=torch.long)\n",
    "\n",
    "        return tokens_tensor, segments_tensor, label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# 初始化一个专门读取训练样本的 Dataset 使用中文 bert 断词\n",
    "trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "想要使用 bert ，就要使用bert能够看的懂的数据，我们需要彻底了解这个Dataset，接下来看一下格式的差异"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['苏', '有', '朋', '要', '结', '婚', '了', '，', '但', '网', '友', '觉', '得', '他', '还', '是', '和', '林', '心', '如', '比', '较', '合', '适']\n",
      "[原始文本]\n",
      "句子1： 苏有朋要结婚了，但网友觉得他还是和林心如比较合适\n",
      "句子2： 好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！\n",
      "分类：  unrelated\n",
      "\n",
      "-------------------\n",
      "\n",
      "[Dataset 回传的 tensors]\n",
      "token_tensor :  tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042,  749, 8024,  852, 5381, 1351,\n",
      "        6230, 2533,  800, 6820, 3221, 1469, 3360, 2552, 1963, 3683, 6772, 1394,\n",
      "        6844,  102, 1962, 7318, 6057, 5310, 2042, 5314,  679, 2042, 3184, 4638,\n",
      "        4912, 2269, 2803, 5709, 4413, 8024,  948, 7450, 4638, 4912, 2269, 2957,\n",
      "        3717, 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102])\n",
      "\n",
      "segment_tensor: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "label_tensor: 2\n",
      "\n",
      "-------------------\n",
      "\n",
      "[还原 token_tensors]\n",
      "[CLS]苏有朋要结婚了，但网友觉得他还是和林心如比较合适[SEP]好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！[SEP]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source path:... <ipython-input-13-cd674cf6e081>\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 0\n",
      "22:29:45.243533 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:29:45.243814 line        27     def __getitem__(self, idx):\n",
      "22:29:45.243861 line        31         else:\n",
      "New var:....... text_a = '苏有朋要结婚了，但网友觉得他还是和林心如比较合适'\n",
      "New var:....... text_b = '好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！'\n",
      "New var:....... label = 'unrelated'\n",
      "22:29:45.244340 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:29:45.244444 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:29:45.244626 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:29:45.245243 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['苏', '有', '朋', '要', '结', '婚', '了', '，', '但', '网...'还', '是', '和', '林', '心', '如', '比', '较', '合', '适']\n",
      "22:29:45.246267 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:29:45.246667 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '和', '林', '心', '如', '比', '较', '合', '适', '[SEP]']\n",
      "22:29:45.246949 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 26\n",
      "22:29:45.247155 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['好', '闺', '蜜', '结', '婚', '给', '不', '婚', '族', '的...'岚', '掉', '水', '里', '笑', '哭', '苏', '有', '朋', '！']\n",
      "22:29:45.247936 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '水', '里', '笑', '哭', '苏', '有', '朋', '！', '[SEP]']\n",
      "22:29:45.248235 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:29:45.248438 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 5722, 3300, 3301, 6206, 5310, 2042, 749, 8...7, 7027, 5010, 1526, 5722, 3300, 3301, 8013, 102]\n",
      "22:29:45.248814 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042... 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102])\n",
      "22:29:45.249046 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:29:45.249824 line        54 \n",
      "22:29:45.250996 return      54 \n",
      "Return value:.. (tensor([ 101, 5722, 3300, 3301, 6206, 5310, 204...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.010885\n"
     ]
    }
   ],
   "source": [
    "# 选择第一个样本\n",
    "sample_idx = 0\n",
    "\n",
    "# 将原始文本拿出做对比\n",
    "text_a, text_b, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# 利用刚才建立好的 Dataset 取出转换后的 id tensors\n",
    "token_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# 将 token_tensor 还原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "# 渲染前后差异\n",
    "print(f\"\"\"[原始文本]\n",
    "句子1： {text_a}\n",
    "句子2： {text_b}\n",
    "分类：  {label}\n",
    "\n",
    "-------------------\n",
    "\n",
    "[Dataset 回传的 tensors]\n",
    "token_tensor :  {token_tensor}\n",
    "\n",
    "segment_tensor: {segments_tensor}\n",
    "\n",
    "label_tensor: {label_tensor}\n",
    "\n",
    "-------------------\n",
    "\n",
    "[还原 token_tensors]\n",
    "{combined_text}\n",
    "\"\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "除了FakeNewDataset数据内容，以下内容为bert服务自己的NLP任务需要彻底搞明白的。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "当作可以一次回传一个mini-batch 的 DataLoader\n",
    "这个 DataLoader 吃上面定一个 `FakeNewsDataset`.\n",
    "回传训练 bert 时会需要 4 个 tensors:\n",
    "- token_tensors: （batch_size, max_seq_len_in_batch）\n",
    "- segment_tensors: (batch_size, max_seq_len_in_batch )\n",
    "- masks_tensors: (batch_size, max_seq_len_in_batch )\n",
    "- label_tensors: (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segment_tensors = [s[0] for s in samples]\n",
    "\n",
    "    # 测试集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "\n",
    "    # zero pad 到同一序列长度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segment_tensors = pad_sequence(segment_tensors, batch_first=True)\n",
    "\n",
    "    # attention masks 将 token_tensors 设置不为 zero padding\n",
    "    # 的位置设为1 让 bert 只需要关注这些位置的tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    return tokens_tensors, segment_tensors, masks_tensors, label_ids\n",
    "\n",
    "# 初始化一个每次回传 64 个训练样本的 DataLoader\n",
    "# 利用`collate_fn` 将 list of samples 合成一个 mini-batch\n",
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 0\n",
      "22:33:34.692344 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.692601 line        27     def __getitem__(self, idx):\n",
      "22:33:34.692651 line        31         else:\n",
      "New var:....... text_a = '苏有朋要结婚了，但网友觉得他还是和林心如比较合适'\n",
      "New var:....... text_b = '好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.693154 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.693240 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.693369 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.693712 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['苏', '有', '朋', '要', '结', '婚', '了', '，', '但', '网...'还', '是', '和', '林', '心', '如', '比', '较', '合', '适']\n",
      "22:33:34.694732 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.695077 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '和', '林', '心', '如', '比', '较', '合', '适', '[SEP]']\n",
      "22:33:34.695334 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 26\n",
      "22:33:34.695707 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['好', '闺', '蜜', '结', '婚', '给', '不', '婚', '族', '的...'岚', '掉', '水', '里', '笑', '哭', '苏', '有', '朋', '！']\n",
      "22:33:34.696582 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '水', '里', '笑', '哭', '苏', '有', '朋', '！', '[SEP]']\n",
      "22:33:34.696925 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:33:34.697099 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 5722, 3300, 3301, 6206, 5310, 2042, 749, 8...7, 7027, 5010, 1526, 5722, 3300, 3301, 8013, 102]\n",
      "22:33:34.697677 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042... 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102])\n",
      "22:33:34.697989 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.698808 line        54 \n",
      "22:33:34.700340 return      54 \n",
      "Return value:.. (tensor([ 101, 5722, 3300, 3301, 6206, 5310, 204...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.010565\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 1\n",
      "22:33:34.703087 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.703168 line        27     def __getitem__(self, idx):\n",
      "22:33:34.703209 line        31         else:\n",
      "New var:....... text_a = '爆料李小璐要成前妻了贾乃亮模仿王宝强一步到位、快刀斩乱麻！'\n",
      "New var:....... text_b = '李小璐要变前妻了？贾乃亮可能效仿王宝强当机立断，快刀斩乱麻！'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:34.703695 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:34.703787 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:34.703863 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.704400 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['爆', '料', '李', '小', '璐', '要', '成', '前', '妻', '了...'步', '到', '位', '、', '快', '刀', '斩', '乱', '麻', '！']\n",
      "22:33:34.705175 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.705433 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '爆', '料', '李', '小', '璐', '要', '成', '前'... '位', '、', '快', '刀', '斩', '乱', '麻', '！', '[SEP]']\n",
      "22:33:34.705702 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 31\n",
      "22:33:34.705917 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['李', '小', '璐', '要', '变', '前', '妻', '了', '？', '贾...'机', '立', '断', '，', '快', '刀', '斩', '乱', '麻', '！']\n",
      "22:33:34.706622 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '爆', '料', '李', '小', '璐', '要', '成', '前'... '断', '，', '快', '刀', '斩', '乱', '麻', '！', '[SEP]']\n",
      "22:33:34.706866 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:33:34.707140 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 4255, 3160, 3330, 2207, 4466, 6206, 2768, ...71, 8024, 2571, 1143, 3168, 744, 7937, 8013, 102]\n",
      "22:33:34.707534 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4255, 3160, 3330, 2207, 4466, 6206...2571, 1143, 3168,  744, 7937,        8013,  102])\n",
      "22:33:34.707783 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.708751 line        54 \n",
      "22:33:34.710468 return      54 \n",
      "Return value:.. (tensor([ 101, 4255, 3160, 3330, 2207, 4466, 620... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010072\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 2\n",
      "22:33:34.713187 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.713330 line        27     def __getitem__(self, idx):\n",
      "22:33:34.713370 line        31         else:\n",
      "New var:....... text_a = '为彩礼，母亲把女儿嫁给陌生男子，十年后再见面，母亲湿了眼眶'\n",
      "New var:....... text_b = '阿姨，不要彩礼是觉得你家穷，给你台阶下，不要以为我嫁不出去！'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.713859 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.713948 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.714019 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.714206 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['为', '彩', '礼', '，', '母', '亲', '把', '女', '儿', '嫁...'再', '见', '面', '，', '母', '亲', '湿', '了', '眼', '眶']\n",
      "22:33:34.714944 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.715289 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '为', '彩', '礼', '，', '母', '亲', '把', '女'... '面', '，', '母', '亲', '湿', '了', '眼', '眶', '[SEP]']\n",
      "22:33:34.715516 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 31\n",
      "22:33:34.715720 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['阿', '姨', '，', '不', '要', '彩', '礼', '是', '觉', '得...'不', '要', '以', '为', '我', '嫁', '不', '出', '去', '！']\n",
      "22:33:34.716484 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '为', '彩', '礼', '，', '母', '亲', '把', '女'... '以', '为', '我', '嫁', '不', '出', '去', '！', '[SEP]']\n",
      "22:33:34.716886 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:33:34.717178 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 711, 2506, 4851, 8024, 3678, 779, 2828, 19...809, 711, 2769, 2063, 679, 1139, 1343, 8013, 102]\n",
      "22:33:34.717550 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  711, 2506, 4851, 8024, 3678,  779...2769, 2063,  679, 1139, 1343,        8013,  102])\n",
      "22:33:34.717781 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.718621 line        54 \n",
      "22:33:34.720040 return      54 \n",
      "Return value:.. (tensor([ 101,  711, 2506, 4851, 8024, 3678,  77... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.009635\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 3\n",
      "22:33:34.722860 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.722934 line        27     def __getitem__(self, idx):\n",
      "22:33:34.722972 line        31         else:\n",
      "New var:....... text_a = '猪油是个宝，一勺猪油等于十副药，先备起来再说'\n",
      "New var:....... text_b = '传承千百的猪油为何变得人人唯恐避之不及？揭开猪油的四大谣言！'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.723465 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.723550 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.723622 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.723741 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['猪', '油', '是', '个', '宝', '，', '一', '勺', '猪', '油...'十', '副', '药', '，', '先', '备', '起', '来', '再', '说']\n",
      "22:33:34.724465 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.724846 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '猪', '油', '是', '个', '宝', '，', '一', '勺'... '药', '，', '先', '备', '起', '来', '再', '说', '[SEP]']\n",
      "22:33:34.725049 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 24\n",
      "22:33:34.725244 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['传', '承', '千', '百', '的', '猪', '油', '为', '何', '变...'揭', '开', '猪', '油', '的', '四', '大', '谣', '言', '！']\n",
      "22:33:34.726073 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '猪', '油', '是', '个', '宝', '，', '一', '勺'... '猪', '油', '的', '四', '大', '谣', '言', '！', '[SEP]']\n",
      "22:33:34.726369 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:33:34.726574 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 4343, 3779, 3221, 702, 2140, 8024, 671, 12...3, 3779, 4638, 1724, 1920, 6469, 6241, 8013, 102]\n",
      "22:33:34.726998 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4343, 3779, 3221,  702, 2140, 8024...       4638, 1724, 1920, 6469, 6241, 8013,  102])\n",
      "22:33:34.727284 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.728083 line        54 \n",
      "22:33:34.729230 return      54 \n",
      "Return value:.. (tensor([ 101, 4343, 3779, 3221,  702, 2140, 802...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008749\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 4\n",
      "22:33:34.731636 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.731811 line        27     def __getitem__(self, idx):\n",
      "22:33:34.731860 line        31         else:\n",
      "New var:....... text_a = '剖析：香椿，为什么会致癌？'\n",
      "New var:....... text_b = '香椿含亚硝酸盐多吃会致癌？测完发现是谣言'\n",
      "New var:....... label = 'disagreed'\n",
      "22:33:34.732317 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 1\n",
      "22:33:34.732406 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(1)\n",
      "22:33:34.732473 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.732700 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['剖', '析', '：', '香', '椿', '，', '为', '什', '么', '会', '致', '癌', '？']\n",
      "22:33:34.733199 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.733658 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '剖', '析', '：', '香', '椿', '，', '为', '什', '么', '会', '致', '癌', '？', '[SEP]']\n",
      "22:33:34.733851 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 15\n",
      "22:33:34.734034 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['香', '椿', '含', '亚', '硝', '酸', '盐', '多', '吃', '会', '致', '癌', '？', '测', '完', '发', '现', '是', '谣', '言']\n",
      "22:33:34.734648 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '剖', '析', '：', '香', '椿', '，', '为', '什'... '？', '测', '完', '发', '现', '是', '谣', '言', '[SEP]']\n",
      "22:33:34.734946 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 21\n",
      "22:33:34.735176 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1189, 3358, 8038, 7676, 3499, 8024, 711, 7...3, 3844, 2130, 1355, 4385, 3221, 6469, 6241, 102]\n",
      "22:33:34.735503 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1189, 3358, 8038, 7676, 3499, 8024... 3844, 2130, 1355, 4385, 3221, 6469, 6241,  102])\n",
      "22:33:34.735647 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.736400 line        54 \n",
      "22:33:34.737301 return      54 \n",
      "Return value:.. (tensor([ 101, 1189, 3358, 8038, 7676, 3499, 802... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(1))\n",
      "Elapsed time: 00:00:00.007413\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 5\n",
      "22:33:34.739078 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.739143 line        27     def __getitem__(self, idx):\n",
      "22:33:34.739180 line        31         else:\n",
      "New var:....... text_a = '你听说过“伪狂犬病”吗？真正的狂犬病人不会学狗叫'\n",
      "New var:....... text_b = '盘点2018第一季度谣言，小孩感染狂犬病发作学狗叫上榜'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.739615 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.739780 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.739848 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.740029 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['你', '听', '说', '过', '[UNK]', '伪', '狂', '犬', '病'...'的', '狂', '犬', '病', '人', '不', '会', '学', '狗', '叫']\n",
      "22:33:34.740671 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.740904 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '听', '说', '过', '[UNK]', '伪', '狂',... '犬', '病', '人', '不', '会', '学', '狗', '叫', '[SEP]']\n",
      "22:33:34.741174 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 26\n",
      "22:33:34.741362 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['盘', '点', '2018', '第', '一', '季', '度', '谣', '言',...'狂', '犬', '病', '发', '作', '学', '狗', '叫', '上', '榜']\n",
      "22:33:34.742088 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '听', '说', '过', '[UNK]', '伪', '狂',... '病', '发', '作', '学', '狗', '叫', '上', '榜', '[SEP]']\n",
      "22:33:34.742314 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 25\n",
      "22:33:34.742579 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 872, 1420, 6432, 6814, 100, 841, 4312, 430...567, 1355, 868, 2110, 4318, 1373, 677, 3528, 102]\n",
      "22:33:34.743052 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  872, 1420, 6432, 6814,  100,  841... 868, 2110, 4318, 1373,         677, 3528,  102])\n",
      "22:33:34.743297 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "22:33:34.744103 line        54 \n",
      "22:33:34.745734 return      54 \n",
      "Return value:.. (tensor([ 101,  872, 1420, 6432, 6814,  100,  84...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008712\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 6\n",
      "22:33:34.747822 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.747888 line        27     def __getitem__(self, idx):\n",
      "22:33:34.747926 line        31         else:\n",
      "New var:....... text_a = '有钱人买房从不选这几层？开发商不会告诉你的秘密，看完才知多坑'\n",
      "New var:....... text_b = '买房别买这几层了，连行内人都被坑了，后会知道太晚'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:34.748365 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:34.748451 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:34.748519 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.748706 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['有', '钱', '人', '买', '房', '从', '不', '选', '这', '几...'的', '秘', '密', '，', '看', '完', '才', '知', '多', '坑']\n",
      "22:33:34.749443 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.749749 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '有', '钱', '人', '买', '房', '从', '不', '选'... '密', '，', '看', '完', '才', '知', '多', '坑', '[SEP]']\n",
      "22:33:34.749954 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 32\n",
      "22:33:34.750144 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['买', '房', '别', '买', '这', '几', '层', '了', '，', '连...'被', '坑', '了', '，', '后', '会', '知', '道', '太', '晚']\n",
      "22:33:34.750793 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '有', '钱', '人', '买', '房', '从', '不', '选'... '了', '，', '后', '会', '知', '道', '太', '晚', '[SEP]']\n",
      "22:33:34.751063 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 25\n",
      "22:33:34.751260 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3300, 7178, 782, 743, 2791, 794, 679, 6848...49, 8024, 1400, 833, 4761, 6887, 1922, 3241, 102]\n",
      "22:33:34.751604 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3300, 7178,  782,  743, 2791,  794... 8024, 1400,  833, 4761, 6887, 1922, 3241,  102])\n",
      "22:33:34.751823 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.752606 line        54 \n",
      "22:33:34.753761 return      54 \n",
      "Return value:.. (tensor([ 101, 3300, 7178,  782,  743, 2791,  79...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008354\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 7\n",
      "22:33:34.756202 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.756263 line        27     def __getitem__(self, idx):\n",
      "22:33:34.756362 line        31         else:\n",
      "New var:....... text_a = '真假？武安某商场家长因孩子争抢玩具大打出手致一人死亡？'\n",
      "New var:....... text_b = '邯郸女子微信发布“老人猥亵女童”谣言被拘留'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.756756 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.756836 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.756894 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.757059 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['真', '假', '？', '武', '安', '某', '商', '场', '家', '长...'大', '打', '出', '手', '致', '一', '人', '死', '亡', '？']\n",
      "22:33:34.757675 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.757952 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '真', '假', '？', '武', '安', '某', '商', '场'... '出', '手', '致', '一', '人', '死', '亡', '？', '[SEP]']\n",
      "22:33:34.758126 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 29\n",
      "22:33:34.758242 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['邯', '郸', '女', '子', '微', '信', '发', '布', '[UNK]'... '亵', '女', '童', '[UNK]', '谣', '言', '被', '拘', '留']\n",
      "22:33:34.758793 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '真', '假', '？', '武', '安', '某', '商', '场'..., '童', '[UNK]', '谣', '言', '被', '拘', '留', '[SEP]']\n",
      "22:33:34.758975 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 22\n",
      "22:33:34.759213 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 4696, 969, 8043, 3636, 2128, 3378, 1555, 1...57, 4997, 100, 6469, 6241, 6158, 2872, 4522, 102]\n",
      "22:33:34.759513 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4696,  969, 8043, 3636, 2128, 3378... 100, 6469, 6241, 6158,        2872, 4522,  102])\n",
      "22:33:34.759710 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "22:33:34.760332 line        54 \n",
      "22:33:34.761399 return      54 \n",
      "Return value:.. (tensor([ 101, 4696,  969, 8043, 3636, 2128, 337...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007344\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 8\n",
      "22:33:34.763572 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.763633 line        27     def __getitem__(self, idx):\n",
      "22:33:34.763667 line        31         else:\n",
      "New var:....... text_a = '其他文玩都在暴跌！为何南红玛瑙价格持续上涨！'\n",
      "New var:....... text_b = '盘点那些无厘头的紫砂壶谣言：豆腐开壶、黄龙山绝矿...'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.764020 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.764099 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.764158 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.764329 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['其', '他', '文', '玩', '都', '在', '暴', '跌', '！', '为...'红', '玛', '瑙', '价', '格', '持', '续', '上', '涨', '！']\n",
      "22:33:34.764875 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.765150 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '其', '他', '文', '玩', '都', '在', '暴', '跌'... '瑙', '价', '格', '持', '续', '上', '涨', '！', '[SEP]']\n",
      "22:33:34.765311 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 24\n",
      "22:33:34.765477 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['盘', '点', '那', '些', '无', '厘', '头', '的', '紫', '砂...'壶', '、', '黄', '龙', '山', '绝', '矿', '.', '.', '.']\n",
      "22:33:34.766091 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '其', '他', '文', '玩', '都', '在', '暴', '跌'... '黄', '龙', '山', '绝', '矿', '.', '.', '.', '[SEP]']\n",
      "22:33:34.766289 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 28\n",
      "22:33:34.766549 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1071, 800, 3152, 4381, 6963, 1762, 3274, 6...7942, 7987, 2255, 5318, 4771, 119, 119, 119, 102]\n",
      "22:33:34.766850 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1071,  800, 3152, 4381, 6963, 1762...2255, 5318, 4771,         119,  119,  119,  102])\n",
      "22:33:34.767046 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "22:33:34.767647 line        54 \n",
      "22:33:34.768752 return      54 \n",
      "Return value:.. (tensor([ 101, 1071,  800, 3152, 4381, 6963, 176...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007328\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 9\n",
      "22:33:34.770927 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.770986 line        27     def __getitem__(self, idx):\n",
      "22:33:34.771020 line        31         else:\n",
      "New var:....... text_a = '「悦读」官方发布！恢复微信聊天记录教程！快点学起来……'\n",
      "New var:....... text_b = '手机操作微信恢复聊天记录，视频教程'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:34.771362 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:34.771440 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:34.771499 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.771682 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['「', '悦', '读', '」', '官', '方', '发', '布', '！', '恢..., '！', '快', '点', '学', '起', '来', '[UNK]', '[UNK]']\n",
      "22:33:34.772298 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.772574 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '「', '悦', '读', '」', '官', '方', '发', '布'...', '点', '学', '起', '来', '[UNK]', '[UNK]', '[SEP]']\n",
      "22:33:34.772733 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 29\n",
      "22:33:34.772875 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['手', '机', '操', '作', '微', '信', '恢', '复', '聊', '天', '记', '录', '，', '视', '频', '教', '程']\n",
      "22:33:34.773332 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '「', '悦', '读', '」', '官', '方', '发', '布'... '天', '记', '录', '，', '视', '频', '教', '程', '[SEP]']\n",
      "22:33:34.773514 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 18\n",
      "22:33:34.773688 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 519, 2643, 6438, 520, 2135, 3175, 1355, 23...1, 6381, 2497, 8024, 6228, 7574, 3136, 4923, 102]\n",
      "22:33:34.774068 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  519, 2643, 6438,  520, 2135, 3175... 6381, 2497, 8024, 6228, 7574, 3136, 4923,  102])\n",
      "22:33:34.774268 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.774839 line        54 \n",
      "22:33:34.775991 return      54 \n",
      "Return value:.. (tensor([ 101,  519, 2643, 6438,  520, 2135, 317... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007058\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 10\n",
      "22:33:34.778012 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.778074 line        27     def __getitem__(self, idx):\n",
      "22:33:34.778108 line        31         else:\n",
      "New var:....... text_a = '每晚睡前洗脸后，阿莫西林配它抹脸，坚持4天皮肤又白又嫩'\n",
      "New var:....... text_b = '阿莫西林配白醋洗洗脸，变白原来真这么厉害，赶紧试一试'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:34.778462 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:34.778542 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:34.778602 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.778772 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['每', '晚', '睡', '前', '洗', '脸', '后', '，', '阿', '莫...'坚', '持', '4', '天', '皮', '肤', '又', '白', '又', '嫩']\n",
      "22:33:34.779392 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.779669 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '每', '晚', '睡', '前', '洗', '脸', '后', '，'... '4', '天', '皮', '肤', '又', '白', '又', '嫩', '[SEP]']\n",
      "22:33:34.779833 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 29\n",
      "22:33:34.779971 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['阿', '莫', '西', '林', '配', '白', '醋', '洗', '洗', '脸...'这', '么', '厉', '害', '，', '赶', '紧', '试', '一', '试']\n",
      "22:33:34.780557 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '每', '晚', '睡', '前', '洗', '脸', '后', '，'... '厉', '害', '，', '赶', '紧', '试', '一', '试', '[SEP]']\n",
      "22:33:34.780741 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 27\n",
      "22:33:34.780983 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3680, 3241, 4717, 1184, 3819, 5567, 1400, ...26, 2154, 8024, 6628, 5165, 6407, 671, 6407, 102]\n",
      "22:33:34.781296 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3680, 3241, 4717, 1184, 3819, 5567... 2154, 8024, 6628, 5165, 6407,  671, 6407,  102])\n",
      "22:33:34.781495 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.782126 line        54 \n",
      "22:33:34.783539 return      54 \n",
      "Return value:.. (tensor([ 101, 3680, 3241, 4717, 1184, 3819, 556...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007720\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 11\n",
      "22:33:34.785758 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.785817 line        27     def __getitem__(self, idx):\n",
      "22:33:34.785850 line        31         else:\n",
      "New var:....... text_a = '玲琅满目的植物油做饭致癌，来告诉你正确的选用窍门'\n",
      "New var:....... text_b = '猪油和转基因植物油，哪个会致癌？'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.786193 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.786270 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.786328 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.786496 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['玲', '琅', '满', '目', '的', '植', '物', '油', '做', '饭...'告', '诉', '你', '正', '确', '的', '选', '用', '窍', '门']\n",
      "22:33:34.787143 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.787432 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '玲', '琅', '满', '目', '的', '植', '物', '油'... '你', '正', '确', '的', '选', '用', '窍', '门', '[SEP]']\n",
      "22:33:34.787598 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 26\n",
      "22:33:34.787765 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['猪', '油', '和', '转', '基', '因', '植', '物', '油', '，', '哪', '个', '会', '致', '癌', '？']\n",
      "22:33:34.788229 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '玲', '琅', '满', '目', '的', '植', '物', '油'... '油', '，', '哪', '个', '会', '致', '癌', '？', '[SEP]']\n",
      "22:33:34.788407 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 17\n",
      "22:33:34.788578 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 4386, 4414, 4007, 4680, 4638, 3490, 4289, ...779, 8024, 1525, 702, 833, 5636, 4617, 8043, 102]\n",
      "22:33:34.788914 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4386, 4414, 4007, 4680, 4638, 3490...       1525,  702,  833, 5636, 4617, 8043,  102])\n",
      "22:33:34.789107 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.789707 line        54 \n",
      "22:33:34.790806 return      54 \n",
      "Return value:.. (tensor([ 101, 4386, 4414, 4007, 4680, 4638, 349... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007055\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 12\n",
      "22:33:34.792837 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.792894 line        27     def __getitem__(self, idx):\n",
      "22:33:34.792927 line        31         else:\n",
      "New var:....... text_a = '明察｜大连警方：正侦破特大杀人案，网传行凶视频与本案无关'\n",
      "New var:....... text_b = '北京将有雷达无法测量的特大狂风暴雨？气象专家辟谣'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.793263 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.793340 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.793398 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.793582 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['明', '察', '｜', '大', '连', '警', '方', '：', '正', '侦...'传', '行', '凶', '视', '频', '与', '本', '案', '无', '关']\n",
      "22:33:34.794216 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.794514 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '察', '｜', '大', '连', '警', '方', '：'... '凶', '视', '频', '与', '本', '案', '无', '关', '[SEP]']\n",
      "22:33:34.794674 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 30\n",
      "22:33:34.794840 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['北', '京', '将', '有', '雷', '达', '无', '法', '测', '量...'风', '暴', '雨', '？', '气', '象', '专', '家', '辟', '谣']\n",
      "22:33:34.795420 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '察', '｜', '大', '连', '警', '方', '：'... '雨', '？', '气', '象', '专', '家', '辟', '谣', '[SEP]']\n",
      "22:33:34.795601 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 25\n",
      "22:33:34.795839 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3209, 2175, 8078, 1920, 6825, 6356, 3175, ...33, 8043, 3698, 6496, 683, 2157, 6792, 6469, 102]\n",
      "22:33:34.796145 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3209, 2175, 8078, 1920, 6825, 6356...       3698, 6496,  683, 2157, 6792, 6469,  102])\n",
      "22:33:34.796342 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.797007 line        54 \n",
      "22:33:34.798153 return      54 \n",
      "Return value:.. (tensor([ 101, 3209, 2175, 8078, 1920, 6825, 635...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007757\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 13\n",
      "22:33:34.800620 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.800681 line        27     def __getitem__(self, idx):\n",
      "22:33:34.800714 line        31         else:\n",
      "New var:....... text_a = '一个补肾的偏方，只需几毛钱的枸杞告别时间短，一次根除'\n",
      "New var:....... text_b = '黄奕前夫喊话马苏，不想被广电局封杀就道歉，马苏这是摊上大事了'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.801070 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.801147 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.801206 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.801437 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['一', '个', '补', '肾', '的', '偏', '方', '，', '只', '需...'告', '别', '时', '间', '短', '，', '一', '次', '根', '除']\n",
      "22:33:34.802041 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.802312 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '个', '补', '肾', '的', '偏', '方', '，'... '时', '间', '短', '，', '一', '次', '根', '除', '[SEP]']\n",
      "22:33:34.802473 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 28\n",
      "22:33:34.802638 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['黄', '奕', '前', '夫', '喊', '话', '马', '苏', '，', '不...'，', '马', '苏', '这', '是', '摊', '上', '大', '事', '了']\n",
      "22:33:34.803296 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '个', '补', '肾', '的', '偏', '方', '，'... '苏', '这', '是', '摊', '上', '大', '事', '了', '[SEP]']\n",
      "22:33:34.803477 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:33:34.803715 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 671, 702, 6133, 5513, 4638, 974, 3175, 802...5722, 6821, 3221, 3033, 677, 1920, 752, 749, 102]\n",
      "22:33:34.804030 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  671,  702, 6133, 5513, 4638,  974... 6821, 3221, 3033,  677, 1920,  752,  749,  102])\n",
      "22:33:34.804233 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.804877 line        54 \n",
      "22:33:34.806166 return      54 \n",
      "Return value:.. (tensor([ 101,  671,  702, 6133, 5513, 4638,  97...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007907\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 14\n",
      "22:33:34.808552 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.808611 line        27     def __getitem__(self, idx):\n",
      "22:33:34.808646 line        31         else:\n",
      "New var:....... text_a = '大蒜加它一起吃，防血栓，延缓衰老，一生不得癌症！'\n",
      "New var:....... text_b = '女人皮肤暗黄、不水润不用慌！常吃8款养颜食物，排毒养颜更护肤'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.808998 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.809074 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.809133 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.809323 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['大', '蒜', '加', '它', '一', '起', '吃', '，', '防', '血...'衰', '老', '，', '一', '生', '不', '得', '癌', '症', '！']\n",
      "22:33:34.809902 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.810227 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', '蒜', '加', '它', '一', '起', '吃', '，'... '，', '一', '生', '不', '得', '癌', '症', '！', '[SEP]']\n",
      "22:33:34.810400 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 26\n",
      "22:33:34.810573 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['女', '人', '皮', '肤', '暗', '黄', '、', '不', '水', '润...'食', '物', '，', '排', '毒', '养', '颜', '更', '护', '肤']\n",
      "22:33:34.811244 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', '蒜', '加', '它', '一', '起', '吃', '，'... '，', '排', '毒', '养', '颜', '更', '护', '肤', '[SEP]']\n",
      "22:33:34.811512 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:33:34.811696 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1920, 5886, 1217, 2124, 671, 6629, 1391, 8...4, 2961, 3681, 1075, 7582, 3291, 2844, 5502, 102]\n",
      "22:33:34.812011 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1920, 5886, 1217, 2124,  671, 6629... 2961, 3681, 1075, 7582, 3291, 2844, 5502,  102])\n",
      "22:33:34.812216 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.812961 line        54 \n",
      "22:33:34.814040 return      54 \n",
      "Return value:.. (tensor([ 101, 1920, 5886, 1217, 2124,  671, 662...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007674\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 15\n",
      "22:33:34.816251 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.816310 line        27     def __getitem__(self, idx):\n",
      "22:33:34.816343 line        31         else:\n",
      "New var:....... text_a = '沙湾有人吃羊肉得了SK6病毒？切！我还SK2呢！'\n",
      "New var:....... text_b = '「辟谣」病毒式网络谣言如何消除？'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.816691 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.816839 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.816901 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.817066 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['沙', '湾', '有', '人', '吃', '羊', '肉', '得', '了', '[... '毒', '？', '切', '！', '我', '还', '[UNK]', '呢', '！']\n",
      "22:33:34.817619 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.817835 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '沙', '湾', '有', '人', '吃', '羊', '肉', '得'..., '切', '！', '我', '还', '[UNK]', '呢', '！', '[SEP]']\n",
      "22:33:34.817995 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 22\n",
      "22:33:34.818224 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['「', '辟', '谣', '」', '病', '毒', '式', '网', '络', '谣', '言', '如', '何', '消', '除', '？']\n",
      "22:33:34.818695 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '沙', '湾', '有', '人', '吃', '羊', '肉', '得'... '络', '谣', '言', '如', '何', '消', '除', '？', '[SEP]']\n",
      "22:33:34.818901 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 17\n",
      "22:33:34.819081 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3763, 3968, 3300, 782, 1391, 5399, 5489, 2...17, 6469, 6241, 1963, 862, 3867, 7370, 8043, 102]\n",
      "22:33:34.819349 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3763, 3968, 3300,  782, 1391, 5399...6241, 1963,  862, 3867,        7370, 8043,  102])\n",
      "22:33:34.819609 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.820164 line        54 \n",
      "22:33:34.821206 return      54 \n",
      "Return value:.. (tensor([ 101, 3763, 3968, 3300,  782, 1391, 539... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.006730\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 16\n",
      "22:33:34.823013 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.823076 line        27     def __getitem__(self, idx):\n",
      "22:33:34.823110 line        31         else:\n",
      "New var:....... text_a = '林更新的一句话疑似支持鹿晗关晓彤分手，网友：也许他们会结婚呢'\n",
      "New var:....... text_b = '哪位明星结婚了？林更新，高圆圆赵又廷都来参加了'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.823511 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.823592 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.823655 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.823873 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['林', '更', '新', '的', '一', '句', '话', '疑', '似', '支...'友', '：', '也', '许', '他', '们', '会', '结', '婚', '呢']\n",
      "22:33:34.824545 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.824862 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '更', '新', '的', '一', '句', '话', '疑'... '也', '许', '他', '们', '会', '结', '婚', '呢', '[SEP]']\n",
      "22:33:34.825043 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 32\n",
      "22:33:34.825219 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['哪', '位', '明', '星', '结', '婚', '了', '？', '林', '更...'圆', '圆', '赵', '又', '廷', '都', '来', '参', '加', '了']\n",
      "22:33:34.825797 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '更', '新', '的', '一', '句', '话', '疑'... '赵', '又', '廷', '都', '来', '参', '加', '了', '[SEP]']\n",
      "22:33:34.825982 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 24\n",
      "22:33:34.826225 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3360, 3291, 3173, 4638, 671, 1368, 6413, 4...27, 1348, 2455, 6963, 3341, 1346, 1217, 749, 102]\n",
      "22:33:34.826538 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3360, 3291, 3173, 4638,  671, 1368... 1348, 2455, 6963, 3341, 1346, 1217,  749,  102])\n",
      "22:33:34.826740 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.827377 line        54 \n",
      "22:33:34.828671 return      54 \n",
      "Return value:.. (tensor([ 101, 3360, 3291, 3173, 4638,  671, 136...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008145\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 17\n",
      "22:33:34.831186 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.831247 line        27     def __getitem__(self, idx):\n",
      "22:33:34.831281 line        31         else:\n",
      "New var:....... text_a = '夏季减肥的你千万别错过这些高效刮油食物哦！'\n",
      "New var:....... text_b = '最减肥的几种食物，消脂刮油，减肥族的首选'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.831657 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.831736 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.831794 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.831964 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['夏', '季', '减', '肥', '的', '你', '千', '万', '别', '错...'这', '些', '高', '效', '刮', '油', '食', '物', '哦', '！']\n",
      "22:33:34.832496 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.832769 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '夏', '季', '减', '肥', '的', '你', '千', '万'... '高', '效', '刮', '油', '食', '物', '哦', '！', '[SEP]']\n",
      "22:33:34.832929 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 23\n",
      "22:33:34.833095 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['最', '减', '肥', '的', '几', '种', '食', '物', '，', '消', '脂', '刮', '油', '，', '减', '肥', '族', '的', '首', '选']\n",
      "22:33:34.833648 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '夏', '季', '减', '肥', '的', '你', '千', '万'... '油', '，', '减', '肥', '族', '的', '首', '选', '[SEP]']\n",
      "22:33:34.833834 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 21\n",
      "22:33:34.834074 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1909, 2108, 1121, 5503, 4638, 872, 1283, 6...9, 8024, 1121, 5503, 3184, 4638, 7674, 6848, 102]\n",
      "22:33:34.834357 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1909, 2108, 1121, 5503, 4638,  872... 8024, 1121, 5503, 3184, 4638, 7674, 6848,  102])\n",
      "22:33:34.834554 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.835114 line        54 \n",
      "22:33:34.836190 return      54 \n",
      "Return value:.. (tensor([ 101, 1909, 2108, 1121, 5503, 4638,  87... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.006894\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 18\n",
      "22:33:34.838133 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.838216 line        27     def __getitem__(self, idx):\n",
      "22:33:34.838251 line        31         else:\n",
      "New var:....... text_a = '廊坊网传飞机撒药治白蛾 市园林局：假消息！'\n",
      "New var:....... text_b = '朋友圈谣传医院将收取26元药事服务费 记者求证系假消息'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.838607 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.838686 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.838745 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.838917 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['廊', '坊', '网', '传', '飞', '机', '撒', '药', '治', '白', '蛾', '市', '园', '林', '局', '：', '假', '消', '息', '！']\n",
      "22:33:34.839440 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.839711 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '廊', '坊', '网', '传', '飞', '机', '撒', '药'... '园', '林', '局', '：', '假', '消', '息', '！', '[SEP]']\n",
      "22:33:34.839868 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 22\n",
      "22:33:34.840073 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['朋', '友', '圈', '谣', '传', '医', '院', '将', '收', '取...'务', '费', '记', '者', '求', '证', '系', '假', '消', '息']\n",
      "22:33:34.840707 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '廊', '坊', '网', '传', '飞', '机', '撒', '药'... '记', '者', '求', '证', '系', '假', '消', '息', '[SEP]']\n",
      "22:33:34.840893 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 26\n",
      "22:33:34.841132 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 2443, 1773, 5381, 837, 7607, 3322, 3054, 5...81, 5442, 3724, 6395, 5143, 969, 3867, 2622, 102]\n",
      "22:33:34.841422 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2443, 1773, 5381,  837, 7607, 3322... 5442, 3724, 6395, 5143,  969, 3867, 2622,  102])\n",
      "22:33:34.841617 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.842194 line        54 \n",
      "22:33:34.843301 return      54 \n",
      "Return value:.. (tensor([ 101, 2443, 1773, 5381,  837, 7607, 332... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007183\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 19\n",
      "22:33:34.845340 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.845398 line        27     def __getitem__(self, idx):\n",
      "22:33:34.845431 line        31         else:\n",
      "New var:....... text_a = '王珂再次破产欠六千万巨款！刘涛又哭了，夫妻一起做节目还债'\n",
      "New var:....... text_b = '《极限挑战》今天来温州大学拍摄？别传了，这是谣言！'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.845779 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.845858 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.846000 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.846193 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['王', '珂', '再', '次', '破', '产', '欠', '六', '千', '万...'，', '夫', '妻', '一', '起', '做', '节', '目', '还', '债']\n",
      "22:33:34.846819 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.847092 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '珂', '再', '次', '破', '产', '欠', '六'... '妻', '一', '起', '做', '节', '目', '还', '债', '[SEP]']\n",
      "22:33:34.847250 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 30\n",
      "22:33:34.847415 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['《', '极', '限', '挑', '战', '》', '今', '天', '来', '温...'？', '别', '传', '了', '，', '这', '是', '谣', '言', '！']\n",
      "22:33:34.848006 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '珂', '再', '次', '破', '产', '欠', '六'... '传', '了', '，', '这', '是', '谣', '言', '！', '[SEP]']\n",
      "22:33:34.848186 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 26\n",
      "22:33:34.848424 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 4374, 4392, 1086, 3613, 4788, 772, 3612, 1...37, 749, 8024, 6821, 3221, 6469, 6241, 8013, 102]\n",
      "22:33:34.848732 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4374, 4392, 1086, 3613, 4788,  772...  749, 8024, 6821, 3221, 6469, 6241, 8013,  102])\n",
      "22:33:34.848927 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.849554 line        54 \n",
      "22:33:34.850823 return      54 \n",
      "Return value:.. (tensor([ 101, 4374, 4392, 1086, 3613, 4788,  77...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007996\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 20\n",
      "22:33:34.853484 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.853571 line        27     def __getitem__(self, idx):\n",
      "22:33:34.853621 line        31         else:\n",
      "New var:....... text_a = '白百合电视剧下架，首次反驳出轨并没错！这样的女星还能理直气壮'\n",
      "New var:....... text_b = '大肚腩难看，减肥专家教你一招瘦肚子，做个小瘦子'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.854002 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.854097 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.854170 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.854388 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['白', '百', '合', '电', '视', '剧', '下', '架', '，', '首...'样', '的', '女', '星', '还', '能', '理', '直', '气', '壮']\n",
      "22:33:34.855024 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.855291 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '白', '百', '合', '电', '视', '剧', '下', '架'... '女', '星', '还', '能', '理', '直', '气', '壮', '[SEP]']\n",
      "22:33:34.855446 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 32\n",
      "22:33:34.855606 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['大', '肚', '腩', '难', '看', '，', '减', '肥', '专', '家...'招', '瘦', '肚', '子', '，', '做', '个', '小', '瘦', '子']\n",
      "22:33:34.856146 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '白', '百', '合', '电', '视', '剧', '下', '架'... '肚', '子', '，', '做', '个', '小', '瘦', '子', '[SEP]']\n",
      "22:33:34.856322 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 24\n",
      "22:33:34.856551 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 4635, 4636, 1394, 4510, 6228, 1196, 678, 3...496, 2094, 8024, 976, 702, 2207, 4607, 2094, 102]\n",
      "22:33:34.856848 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4635, 4636, 1394, 4510, 6228, 1196... 2094, 8024,  976,  702, 2207, 4607, 2094,  102])\n",
      "22:33:34.857038 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.857913 line        54 \n",
      "22:33:34.859208 return      54 \n",
      "Return value:.. (tensor([ 101, 4635, 4636, 1394, 4510, 6228, 119...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008464\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 21\n",
      "22:33:34.861977 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.862055 line        27     def __getitem__(self, idx):\n",
      "22:33:34.862087 line        31         else:\n",
      "New var:....... text_a = '微信又出新功能了以后用微信可以免费上网了'\n",
      "New var:....... text_b = '微信新功能，可以免费上网，只需这样设置下微信'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:34.862424 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:34.862498 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:34.862553 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.862744 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['微', '信', '又', '出', '新', '功', '能', '了', '以', '后', '用', '微', '信', '可', '以', '免', '费', '上', '网', '了']\n",
      "22:33:34.863246 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.863515 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '微', '信', '又', '出', '新', '功', '能', '了'... '信', '可', '以', '免', '费', '上', '网', '了', '[SEP]']\n",
      "22:33:34.863712 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 22\n",
      "22:33:34.863914 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['微', '信', '新', '功', '能', '，', '可', '以', '免', '费...'，', '只', '需', '这', '样', '设', '置', '下', '微', '信']\n",
      "22:33:34.864484 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '微', '信', '又', '出', '新', '功', '能', '了'... '需', '这', '样', '设', '置', '下', '微', '信', '[SEP]']\n",
      "22:33:34.865036 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 23\n",
      "22:33:34.865259 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 2544, 928, 1348, 1139, 3173, 1216, 5543, 7...444, 6821, 3416, 6392, 5390, 678, 2544, 928, 102]\n",
      "22:33:34.865644 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2544,  928, 1348, 1139, 3173, 1216... 6821, 3416, 6392, 5390,  678, 2544,  928,  102])\n",
      "22:33:34.865831 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.866510 line        54 \n",
      "22:33:34.868309 return      54 \n",
      "Return value:.. (tensor([ 101, 2544,  928, 1348, 1139, 3173, 121... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008720\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 22\n",
      "22:33:34.870704 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.870765 line        27     def __getitem__(self, idx):\n",
      "22:33:34.870799 line        31         else:\n",
      "New var:....... text_a = '张一山患抑郁症将退出娱乐圈？回应：暂时死不了，拍完戏就休息'\n",
      "New var:....... text_b = '曝！张一山身体出现问题\\xa0未来可能退出娱乐圈'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.871173 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.871252 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.871313 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.871563 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['张', '一', '山', '患', '抑', '郁', '症', '将', '退', '出...'死', '不', '了', '，', '拍', '完', '戏', '就', '休', '息']\n",
      "22:33:34.872247 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.872531 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '一', '山', '患', '抑', '郁', '症', '将'... '了', '，', '拍', '完', '戏', '就', '休', '息', '[SEP]']\n",
      "22:33:34.872711 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 31\n",
      "22:33:34.872906 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['曝', '！', '张', '一', '山', '身', '体', '出', '现', '问', '题', '未', '来', '可', '能', '退', '出', '娱', '乐', '圈']\n",
      "22:33:34.873473 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '一', '山', '患', '抑', '郁', '症', '将'... '来', '可', '能', '退', '出', '娱', '乐', '圈', '[SEP]']\n",
      "22:33:34.873653 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 21\n",
      "22:33:34.873889 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 2476, 671, 2255, 2642, 2829, 6944, 4568, 2...41, 1377, 5543, 6842, 1139, 2031, 727, 1750, 102]\n",
      "22:33:34.874192 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476,  671, 2255, 2642, 2829, 6944...5543, 6842, 1139,        2031,  727, 1750,  102])\n",
      "22:33:34.874386 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "22:33:34.874989 line        54 \n",
      "22:33:34.876319 return      54 \n",
      "Return value:.. (tensor([ 101, 2476,  671, 2255, 2642, 2829, 694...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007953\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 23\n",
      "22:33:34.878689 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.878753 line        27     def __getitem__(self, idx):\n",
      "22:33:34.878786 line        31         else:\n",
      "New var:....... text_a = '冰淇淋融化的越慢，添加剂就越多吗？'\n",
      "New var:....... text_b = '每天都滴2滴风油精在肚脐，一个星期后身体竟发生了奇妙的变化'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.879174 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.879251 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.879311 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.879601 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['冰', '淇', '淋', '融', '化', '的', '越', '慢', '，', '添', '加', '剂', '就', '越', '多', '吗', '？']\n",
      "22:33:34.880065 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.880375 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '冰', '淇', '淋', '融', '化', '的', '越', '慢'... '添', '加', '剂', '就', '越', '多', '吗', '？', '[SEP]']\n",
      "22:33:34.880529 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 19\n",
      "22:33:34.880699 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['每', '天', '都', '滴', '2', '滴', '风', '油', '精', '在...'体', '竟', '发', '生', '了', '奇', '妙', '的', '变', '化']\n",
      "22:33:34.881356 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '冰', '淇', '淋', '融', '化', '的', '越', '慢'... '发', '生', '了', '奇', '妙', '的', '变', '化', '[SEP]']\n",
      "22:33:34.881558 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 30\n",
      "22:33:34.881800 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1102, 3899, 3900, 6084, 1265, 4638, 6632, ...55, 4495, 749, 1936, 1975, 4638, 1359, 1265, 102]\n",
      "22:33:34.882089 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1102, 3899, 3900, 6084, 1265, 4638... 749, 1936, 1975, 4638, 1359, 1265,         102])\n",
      "22:33:34.882280 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "22:33:34.882861 line        54 \n",
      "22:33:34.883940 return      54 \n",
      "Return value:.. (tensor([ 101, 1102, 3899, 3900, 6084, 1265, 463...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008154\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 24\n",
      "22:33:34.886872 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.886933 line        27     def __getitem__(self, idx):\n",
      "22:33:34.886966 line        31         else:\n",
      "New var:....... text_a = '美国公布拷问抓到的外星人的视频，称自己来自未来'\n",
      "New var:....... text_b = '美国终于公布了活着的外星人的视频！你被惊到了吗？'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.887364 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.887440 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.887499 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.887688 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['美', '国', '公', '布', '拷', '问', '抓', '到', '的', '外...'视', '频', '，', '称', '自', '己', '来', '自', '未', '来']\n",
      "22:33:34.888276 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.888548 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '美', '国', '公', '布', '拷', '问', '抓', '到'... '，', '称', '自', '己', '来', '自', '未', '来', '[SEP]']\n",
      "22:33:34.888703 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 25\n",
      "22:33:34.888863 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['美', '国', '终', '于', '公', '布', '了', '活', '着', '的...'视', '频', '！', '你', '被', '惊', '到', '了', '吗', '？']\n",
      "22:33:34.889430 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '美', '国', '公', '布', '拷', '问', '抓', '到'... '！', '你', '被', '惊', '到', '了', '吗', '？', '[SEP]']\n",
      "22:33:34.889605 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 25\n",
      "22:33:34.889835 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 5401, 1744, 1062, 2357, 2893, 7309, 2831, ...013, 872, 6158, 2661, 1168, 749, 1408, 8043, 102]\n",
      "22:33:34.890121 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5401, 1744, 1062, 2357, 2893, 7309...6158, 2661, 1168,  749, 1408,        8043,  102])\n",
      "22:33:34.890310 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "22:33:34.891030 line        54 \n",
      "22:33:34.892503 return      54 \n",
      "Return value:.. (tensor([ 101, 5401, 1744, 1062, 2357, 2893, 730...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008516\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 25\n",
      "22:33:34.895418 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.895519 line        27     def __getitem__(self, idx):\n",
      "22:33:34.895553 line        31         else:\n",
      "New var:....... text_a = '买房万万别选这几种房子，很多人没听劝，交钱后肠子都悔青了'\n",
      "New var:....... text_b = '越来越多人买房不选这3个楼层了，听老师傅一说，后悔我家买早了'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.895966 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.896044 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.896118 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.896455 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['买', '房', '万', '万', '别', '选', '这', '几', '种', '房...'，', '交', '钱', '后', '肠', '子', '都', '悔', '青', '了']\n",
      "22:33:34.897084 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.897447 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '买', '房', '万', '万', '别', '选', '这', '几'... '钱', '后', '肠', '子', '都', '悔', '青', '了', '[SEP]']\n",
      "22:33:34.897674 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 30\n",
      "22:33:34.897887 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['越', '来', '越', '多', '人', '买', '房', '不', '选', '这...'一', '说', '，', '后', '悔', '我', '家', '买', '早', '了']\n",
      "22:33:34.898853 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '买', '房', '万', '万', '别', '选', '这', '几'... '，', '后', '悔', '我', '家', '买', '早', '了', '[SEP]']\n",
      "22:33:34.899312 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:33:34.899539 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 743, 2791, 674, 674, 1166, 6848, 6821, 112...024, 1400, 2637, 2769, 2157, 743, 3193, 749, 102]\n",
      "22:33:34.899905 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  743, 2791,  674,  674, 1166, 6848...2637, 2769, 2157,  743, 3193,  749,         102])\n",
      "22:33:34.900155 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.901313 line        54 \n",
      "22:33:34.902670 return      54 \n",
      "Return value:.. (tensor([ 101,  743, 2791,  674,  674, 1166, 684... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.010031\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 26\n",
      "22:33:34.905483 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.905548 line        27     def __getitem__(self, idx):\n",
      "22:33:34.905605 line        31         else:\n",
      "New var:....... text_a = '付笛声、任静夫妻皈依佛门？一曲知心爱人重新回忆他们的爱情！'\n",
      "New var:....... text_b = '付笛生夫妇近照，两人皈依佛门不出家，四合院豪宅价值数亿'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.906187 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.906264 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.906460 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.906879 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['付', '笛', '声', '、', '任', '静', '夫', '妻', '皈', '依...'重', '新', '回', '忆', '他', '们', '的', '爱', '情', '！']\n",
      "22:33:34.907958 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.909010 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '付', '笛', '声', '、', '任', '静', '夫', '妻'... '回', '忆', '他', '们', '的', '爱', '情', '！', '[SEP]']\n",
      "22:33:34.909627 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 31\n",
      "22:33:34.910149 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['付', '笛', '生', '夫', '妇', '近', '照', '，', '两', '人...'，', '四', '合', '院', '豪', '宅', '价', '值', '数', '亿']\n",
      "22:33:34.911018 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '付', '笛', '声', '、', '任', '静', '夫', '妻'... '合', '院', '豪', '宅', '价', '值', '数', '亿', '[SEP]']\n",
      "22:33:34.911372 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 28\n",
      "22:33:34.911596 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 802, 5013, 1898, 510, 818, 7474, 1923, 198...1394, 7368, 6498, 2125, 817, 966, 3144, 783, 102]\n",
      "22:33:34.911955 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  802, 5013, 1898,  510,  818, 7474... 7368, 6498, 2125,  817,  966, 3144,  783,  102])\n",
      "22:33:34.912307 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.913189 line        54 \n",
      "22:33:34.914832 return      54 \n",
      "Return value:.. (tensor([ 101,  802, 5013, 1898,  510,  818, 747...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.013257\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 27\n",
      "22:33:34.918769 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.918828 line        27     def __getitem__(self, idx):\n",
      "22:33:34.918860 line        31         else:\n",
      "New var:....... text_a = '嘲讽农民歌曲，那英公然放言刀郎砸电视！'\n",
      "New var:....... text_b = '刀郎终于上春晚了！某英隔空喊话：他要上春晚，我就砸电视'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.919279 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.919355 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.919413 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.919626 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['嘲', '讽', '农', '民', '歌', '曲', '，', '那', '英', '公', '然', '放', '言', '刀', '郎', '砸', '电', '视', '！']\n",
      "22:33:34.920173 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.920741 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '嘲', '讽', '农', '民', '歌', '曲', '，', '那'... '放', '言', '刀', '郎', '砸', '电', '视', '！', '[SEP]']\n",
      "22:33:34.921080 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 21\n",
      "22:33:34.921320 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['刀', '郎', '终', '于', '上', '春', '晚', '了', '！', '某...'要', '上', '春', '晚', '，', '我', '就', '砸', '电', '视']\n",
      "22:33:34.922058 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '嘲', '讽', '农', '民', '歌', '曲', '，', '那'... '春', '晚', '，', '我', '就', '砸', '电', '视', '[SEP]']\n",
      "22:33:34.922335 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 28\n",
      "22:33:34.922554 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1672, 6391, 1093, 3696, 3625, 3289, 8024, ...7, 3241, 8024, 2769, 2218, 4790, 4510, 6228, 102]\n",
      "22:33:34.922886 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1672, 6391, 1093, 3696, 3625, 3289...8024, 2769, 2218, 4790, 4510, 6228,         102])\n",
      "22:33:34.923123 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "22:33:34.925128 line        54 \n",
      "22:33:34.927138 return      54 \n",
      "Return value:.. (tensor([ 101, 1672, 6391, 1093, 3696, 3625, 328...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(2))\n",
      "Elapsed time: 00:00:00.011562\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 28\n",
      "22:33:34.930364 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.930430 line        27     def __getitem__(self, idx):\n",
      "22:33:34.930465 line        31         else:\n",
      "New var:....... text_a = '茄子嫁接西红柿技术'\n",
      "New var:....... text_b = '神奇的嫁接术\\xa0\\xa0\\xa0茄子嫁接西红柿'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:34.930931 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:34.931013 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:34.931208 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.931424 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['茄', '子', '嫁', '接', '西', '红', '柿', '技', '术']\n",
      "22:33:34.932002 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.932354 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '茄', '子', '嫁', '接', '西', '红', '柿', '技', '术', '[SEP]']\n",
      "22:33:34.932467 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 11\n",
      "22:33:34.932583 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['神', '奇', '的', '嫁', '接', '术', '茄', '子', '嫁', '接', '西', '红', '柿']\n",
      "22:33:34.933018 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '茄', '子', '嫁', '接', '西', '红', '柿', '技'... '术', '茄', '子', '嫁', '接', '西', '红', '柿', '[SEP]']\n",
      "22:33:34.933314 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 14\n",
      "22:33:34.933879 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 5746, 2094, 2063, 2970, 6205, 5273, 3398, ...8, 5746, 2094, 2063, 2970, 6205, 5273, 3398, 102]\n",
      "22:33:34.934248 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5746, 2094, 2063, 2970, 6205, 5273...2094, 2063, 2970, 6205, 5273, 3398,         102])\n",
      "22:33:34.934650 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "22:33:34.935497 line        54 \n",
      "22:33:34.937249 return      54 \n",
      "Return value:.. (tensor([ 101, 5746, 2094, 2063, 2970, 6205, 527...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008690\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 29\n",
      "22:33:34.939089 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.939159 line        27     def __getitem__(self, idx):\n",
      "22:33:34.939197 line        31         else:\n",
      "New var:....... text_a = '40岁男人肾虚，每天吃点，补充雄激素，年轻20岁！'\n",
      "New var:....... text_b = '小便次数多，说明肾不好？这18个谣言临沂人别信啦！'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.939711 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.939800 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.939876 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.940197 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['40', '岁', '男', '人', '肾', '虚', '，', '每', '天', '...充', '雄', '激', '素', '，', '年', '轻', '20', '岁', '！']\n",
      "22:33:34.941183 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.941522 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '40', '岁', '男', '人', '肾', '虚', '，', '每...'激', '素', '，', '年', '轻', '20', '岁', '！', '[SEP]']\n",
      "22:33:34.941913 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 25\n",
      "22:33:34.942328 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['小', '便', '次', '数', '多', '，', '说', '明', '肾', '不...'个', '谣', '言', '临', '沂', '人', '别', '信', '啦', '！']\n",
      "22:33:34.943082 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '40', '岁', '男', '人', '肾', '虚', '，', '每... '言', '临', '沂', '人', '别', '信', '啦', '！', '[SEP]']\n",
      "22:33:34.943535 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 25\n",
      "22:33:34.943809 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 8164, 2259, 4511, 782, 5513, 5994, 8024, 3...6241, 707, 3752, 782, 1166, 928, 1568, 8013, 102]\n",
      "22:33:34.944205 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8164, 2259, 4511,  782, 5513, 5994...3752,  782, 1166,  928, 1568,        8013,  102])\n",
      "22:33:34.944608 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "22:33:34.945539 line        54 \n",
      "22:33:34.946958 return      54 \n",
      "Return value:.. (tensor([ 101, 8164, 2259, 4511,  782, 5513, 599...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.011122\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 30\n",
      "22:33:34.950244 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.950312 line        27     def __getitem__(self, idx):\n",
      "22:33:34.950351 line        31         else:\n",
      "New var:....... text_a = '外星人疑是地球人祖先，科学家似发现其尸体，构造和人类多处相似'\n",
      "New var:....... text_b = '美太空舰队在小行星上捕获的新物种外星人，竟然和人构造大致相同'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.950825 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.950915 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.951115 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.951344 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['外', '星', '人', '疑', '是', '地', '球', '人', '祖', '先...'，', '构', '造', '和', '人', '类', '多', '处', '相', '似']\n",
      "22:33:34.952324 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.952620 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '外', '星', '人', '疑', '是', '地', '球', '人'... '造', '和', '人', '类', '多', '处', '相', '似', '[SEP]']\n",
      "22:33:34.952850 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 32\n",
      "22:33:34.953205 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['美', '太', '空', '舰', '队', '在', '小', '行', '星', '上...'竟', '然', '和', '人', '构', '造', '大', '致', '相', '同']\n",
      "22:33:34.954000 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '外', '星', '人', '疑', '是', '地', '球', '人'... '和', '人', '构', '造', '大', '致', '相', '同', '[SEP]']\n",
      "22:33:34.954258 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:33:34.954798 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1912, 3215, 782, 4542, 3221, 1765, 4413, 7...69, 782, 3354, 6863, 1920, 5636, 4685, 1398, 102]\n",
      "22:33:34.955231 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1912, 3215,  782, 4542, 3221, 1765...3354, 6863, 1920, 5636,        4685, 1398,  102])\n",
      "22:33:34.955532 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.956394 line        54 \n",
      "22:33:34.958175 return      54 \n",
      "Return value:.. (tensor([ 101, 1912, 3215,  782, 4542, 3221, 176... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.011449\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 31\n",
      "22:33:34.961728 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.961799 line        27     def __getitem__(self, idx):\n",
      "22:33:34.961837 line        31         else:\n",
      "New var:....... text_a = '华为手机进货价曝光，看看你买的手机多花了多少钱？'\n",
      "New var:....... text_b = '华为回应驻美前员工：无事实依据华为要求员工窃取机密为造谣'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.962309 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.962396 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.962465 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.962703 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['华', '为', '手', '机', '进', '货', '价', '曝', '光', '，...'的', '手', '机', '多', '花', '了', '多', '少', '钱', '？']\n",
      "22:33:34.963520 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.963860 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '华', '为', '手', '机', '进', '货', '价', '曝'... '机', '多', '花', '了', '多', '少', '钱', '？', '[SEP]']\n",
      "22:33:34.963979 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 26\n",
      "22:33:34.964097 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['华', '为', '回', '应', '驻', '美', '前', '员', '工', '：...'求', '员', '工', '窃', '取', '机', '密', '为', '造', '谣']\n",
      "22:33:34.965123 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '华', '为', '手', '机', '进', '货', '价', '曝'... '工', '窃', '取', '机', '密', '为', '造', '谣', '[SEP]']\n",
      "22:33:34.965394 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 29\n",
      "22:33:34.965644 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1290, 711, 2797, 3322, 6822, 6573, 817, 32...39, 4961, 1357, 3322, 2166, 711, 6863, 6469, 102]\n",
      "22:33:34.966197 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1290,  711, 2797, 3322, 6822, 6573...       1357, 3322, 2166,  711, 6863, 6469,  102])\n",
      "22:33:34.966494 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.967409 line        54 \n",
      "22:33:34.968826 return      54 \n",
      "Return value:.. (tensor([ 101, 1290,  711, 2797, 3322, 6822, 657...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.009734\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 32\n",
      "22:33:34.971491 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.971555 line        27     def __getitem__(self, idx):\n",
      "22:33:34.971591 line        31         else:\n",
      "New var:....... text_a = 'TFBOYS要解散?王源已成立个人工作室'\n",
      "New var:....... text_b = 'TFboys单人秀太尴尬被传解散'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.972019 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.972104 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.972166 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.972448 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['[UNK]', '要', '解', '散', '?', '王', '源', '已', '成', '立', '个', '人', '工', '作', '室']\n",
      "22:33:34.972980 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.973441 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '[UNK]', '要', '解', '散', '?', '王', '源', '已', '成', '立', '个', '人', '工', '作', '室', '[SEP]']\n",
      "22:33:34.973666 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 17\n",
      "22:33:34.973899 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['[UNK]', '单', '人', '秀', '太', '尴', '尬', '被', '传', '解', '散']\n",
      "22:33:34.974474 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '[UNK]', '要', '解', '散', '?', '王', '源',... '秀', '太', '尴', '尬', '被', '传', '解', '散', '[SEP]']\n",
      "22:33:34.974834 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 12\n",
      "22:33:34.975008 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 100, 6206, 6237, 3141, 136, 4374, 3975, 23...99, 1922, 2219, 2217, 6158, 837, 6237, 3141, 102]\n",
      "22:33:34.975211 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  100, 6206, 6237, 3141,  136, 4374...2219, 2217,        6158,  837, 6237, 3141,  102])\n",
      "22:33:34.975379 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "22:33:34.975951 line        54 \n",
      "22:33:34.977777 return      54 \n",
      "Return value:.. (tensor([ 101,  100, 6206, 6237, 3141,  136, 437...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008913\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 33\n",
      "22:33:34.980440 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.980514 line        27     def __getitem__(self, idx):\n",
      "22:33:34.980553 line        31         else:\n",
      "New var:....... text_a = '李天一获超快减刑提前6年出狱，梦鸽满面春光'\n",
      "New var:....... text_b = '李天一提前6年出狱？北京监狱辟谣：仍在服刑'\n",
      "New var:....... label = 'disagreed'\n",
      "22:33:34.981043 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 1\n",
      "22:33:34.981132 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(1)\n",
      "22:33:34.981377 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.981928 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['李', '天', '一', '获', '超', '快', '减', '刑', '提', '前...'年', '出', '狱', '，', '梦', '鸽', '满', '面', '春', '光']\n",
      "22:33:34.982753 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.983045 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '天', '一', '获', '超', '快', '减', '刑'... '狱', '，', '梦', '鸽', '满', '面', '春', '光', '[SEP]']\n",
      "22:33:34.983428 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 23\n",
      "22:33:34.983706 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['李', '天', '一', '提', '前', '6', '年', '出', '狱', '？...'京', '监', '狱', '辟', '谣', '：', '仍', '在', '服', '刑']\n",
      "22:33:34.984389 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '天', '一', '获', '超', '快', '减', '刑'... '狱', '辟', '谣', '：', '仍', '在', '服', '刑', '[SEP]']\n",
      "22:33:34.984765 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 22\n",
      "22:33:34.985010 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3330, 1921, 671, 5815, 6631, 2571, 1121, 1...28, 6792, 6469, 8038, 793, 1762, 3302, 1152, 102]\n",
      "22:33:34.985375 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 1921,  671, 5815, 6631, 2571... 6792, 6469, 8038,  793, 1762, 3302, 1152,  102])\n",
      "22:33:34.985645 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.986520 line        54 \n",
      "22:33:34.987827 return      54 \n",
      "Return value:.. (tensor([ 101, 3330, 1921,  671, 5815, 6631, 257... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(1))\n",
      "Elapsed time: 00:00:00.010073\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 34\n",
      "22:33:34.990545 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:34.990611 line        27     def __getitem__(self, idx):\n",
      "22:33:34.990649 line        31         else:\n",
      "New var:....... text_a = '很想儿女双全，想知道怎样才能怀上男孩？如何生儿子的秘诀'\n",
      "New var:....... text_b = '为何林志颖一直生儿子，贾静雯一直生女儿，控制生男生女的秘诀'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:34.991111 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:34.991197 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:34.991262 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:34.991525 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['很', '想', '儿', '女', '双', '全', '，', '想', '知', '道...'孩', '？', '如', '何', '生', '儿', '子', '的', '秘', '诀']\n",
      "22:33:34.992388 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:34.992710 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '很', '想', '儿', '女', '双', '全', '，', '想'... '如', '何', '生', '儿', '子', '的', '秘', '诀', '[SEP]']\n",
      "22:33:34.992945 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 29\n",
      "22:33:34.993293 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['为', '何', '林', '志', '颖', '一', '直', '生', '儿', '子...'，', '控', '制', '生', '男', '生', '女', '的', '秘', '诀']\n",
      "22:33:34.993969 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '很', '想', '儿', '女', '双', '全', '，', '想'... '制', '生', '男', '生', '女', '的', '秘', '诀', '[SEP]']\n",
      "22:33:34.994109 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 30\n",
      "22:33:34.994423 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 2523, 2682, 1036, 1957, 1352, 1059, 8024, ...9, 4495, 4511, 4495, 1957, 4638, 4908, 6394, 102]\n",
      "22:33:34.994877 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2523, 2682, 1036, 1957, 1352, 1059... 4495, 4511, 4495, 1957, 4638, 4908, 6394,  102])\n",
      "22:33:34.995289 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:34.996401 line        54 \n",
      "22:33:34.997894 return      54 \n",
      "Return value:.. (tensor([ 101, 2523, 2682, 1036, 1957, 1352, 105...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.010761\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 35\n",
      "22:33:35.001345 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.001419 line        27     def __getitem__(self, idx):\n",
      "22:33:35.001457 line        31         else:\n",
      "New var:....... text_a = '一把八角，专治前列腺，七天断根，一辈子不反复'\n",
      "New var:....... text_b = '一把八角，专治前列腺，4天断根，一辈子不反复！'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.001934 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.002022 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.002237 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.002699 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['一', '把', '八', '角', '，', '专', '治', '前', '列', '腺...'天', '断', '根', '，', '一', '辈', '子', '不', '反', '复']\n",
      "22:33:35.003371 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.003658 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '把', '八', '角', '，', '专', '治', '前'... '根', '，', '一', '辈', '子', '不', '反', '复', '[SEP]']\n",
      "22:33:35.003992 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 24\n",
      "22:33:35.004225 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['一', '把', '八', '角', '，', '专', '治', '前', '列', '腺...'断', '根', '，', '一', '辈', '子', '不', '反', '复', '！']\n",
      "22:33:35.004909 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '把', '八', '角', '，', '专', '治', '前'... '，', '一', '辈', '子', '不', '反', '复', '！', '[SEP]']\n",
      "22:33:35.005159 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 24\n",
      "22:33:35.005519 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 671, 2828, 1061, 6235, 8024, 683, 3780, 11...024, 671, 6777, 2094, 679, 1353, 1908, 8013, 102]\n",
      "22:33:35.005886 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  671, 2828, 1061, 6235, 8024,  683...  671, 6777, 2094,  679, 1353, 1908, 8013,  102])\n",
      "22:33:35.006149 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.006997 line        54 \n",
      "22:33:35.008070 return      54 \n",
      "Return value:.. (tensor([ 101,  671, 2828, 1061, 6235, 8024,  68... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010006\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 36\n",
      "22:33:35.011390 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.011466 line        27     def __getitem__(self, idx):\n",
      "22:33:35.011505 line        31         else:\n",
      "New var:....... text_a = '老太好心救下一条白蛇，企料那蛇却咬死了老太的儿子'\n",
      "New var:....... text_b = '哑巴老太三救小白蛇，洪水来临时哑巴却开口说话：一命还恩'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.011994 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.012085 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.012165 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.012445 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['老', '太', '好', '心', '救', '下', '一', '条', '白', '蛇...'蛇', '却', '咬', '死', '了', '老', '太', '的', '儿', '子']\n",
      "22:33:35.013328 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.013624 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '老', '太', '好', '心', '救', '下', '一', '条'... '咬', '死', '了', '老', '太', '的', '儿', '子', '[SEP]']\n",
      "22:33:35.013859 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 26\n",
      "22:33:35.014276 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['哑', '巴', '老', '太', '三', '救', '小', '白', '蛇', '，...'却', '开', '口', '说', '话', '：', '一', '命', '还', '恩']\n",
      "22:33:35.015208 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '老', '太', '好', '心', '救', '下', '一', '条'... '口', '说', '话', '：', '一', '命', '还', '恩', '[SEP]']\n",
      "22:33:35.015694 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 28\n",
      "22:33:35.015951 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 5439, 1922, 1962, 2552, 3131, 678, 671, 33...66, 6432, 6413, 8038, 671, 1462, 6820, 2617, 102]\n",
      "22:33:35.016369 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5439, 1922, 1962, 2552, 3131,  678...6413,        8038,  671, 1462, 6820, 2617,  102])\n",
      "22:33:35.016685 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "22:33:35.017606 line        54 \n",
      "22:33:35.019206 return      54 \n",
      "Return value:.. (tensor([ 101, 5439, 1922, 1962, 2552, 3131,  67...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.010277\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 37\n",
      "22:33:35.021697 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.021763 line        27     def __getitem__(self, idx):\n",
      "22:33:35.021801 line        31         else:\n",
      "New var:....... text_a = '又一主持人癌症复发！脸部变形不得已离开舞台，网友：心疼'\n",
      "New var:....... text_b = '又一主持人癌症复发\\xa0脸部变形不得已离开舞台\\xa0网友让人心疼'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.022216 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.022417 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.022487 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.022674 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['又', '一', '主', '持', '人', '癌', '症', '复', '发', '！...'离', '开', '舞', '台', '，', '网', '友', '：', '心', '疼']\n",
      "22:33:35.023367 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.023718 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '又', '一', '主', '持', '人', '癌', '症', '复'... '舞', '台', '，', '网', '友', '：', '心', '疼', '[SEP]']\n",
      "22:33:35.023903 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 29\n",
      "22:33:35.024088 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['又', '一', '主', '持', '人', '癌', '症', '复', '发', '脸...'离', '开', '舞', '台', '网', '友', '让', '人', '心', '疼']\n",
      "22:33:35.024815 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '又', '一', '主', '持', '人', '癌', '症', '复'... '舞', '台', '网', '友', '让', '人', '心', '疼', '[SEP]']\n",
      "22:33:35.025066 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 27\n",
      "22:33:35.025296 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1348, 671, 712, 2898, 782, 4617, 4568, 190...59, 1378, 5381, 1351, 6375, 782, 2552, 4563, 102]\n",
      "22:33:35.025666 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1348,  671,  712, 2898,  782, 4617... 1378, 5381, 1351, 6375,  782, 2552, 4563,  102])\n",
      "22:33:35.025890 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.026839 line        54 \n",
      "22:33:35.028013 return      54 \n",
      "Return value:.. (tensor([ 101, 1348,  671,  712, 2898,  782, 461...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008601\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 38\n",
      "22:33:35.030323 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.030383 line        27     def __getitem__(self, idx):\n",
      "22:33:35.030416 line        31         else:\n",
      "New var:....... text_a = '清朝末年广西僵尸袭人事件是真是假？官府亲自出动镇压'\n",
      "New var:....... text_b = '探究僵尸袭人的未解之谜'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.030882 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.030960 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.031018 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.031179 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['清', '朝', '末', '年', '广', '西', '僵', '尸', '袭', '人...'假', '？', '官', '府', '亲', '自', '出', '动', '镇', '压']\n",
      "22:33:35.031756 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.032059 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '清', '朝', '末', '年', '广', '西', '僵', '尸'... '官', '府', '亲', '自', '出', '动', '镇', '压', '[SEP]']\n",
      "22:33:35.032229 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 27\n",
      "22:33:35.032397 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['探', '究', '僵', '尸', '袭', '人', '的', '未', '解', '之', '谜']\n",
      "22:33:35.032791 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '清', '朝', '末', '年', '广', '西', '僵', '尸'... '尸', '袭', '人', '的', '未', '解', '之', '谜', '[SEP]']\n",
      "22:33:35.032980 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 12\n",
      "22:33:35.033151 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3926, 3308, 3314, 2399, 2408, 6205, 1018, ...221, 6159, 782, 4638, 3313, 6237, 722, 6466, 102]\n",
      "22:33:35.033475 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3926, 3308, 3314, 2399, 2408, 6205... 782, 4638, 3313, 6237,         722, 6466,  102])\n",
      "22:33:35.033664 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.034286 line        54 \n",
      "22:33:35.035257 return      54 \n",
      "Return value:.. (tensor([ 101, 3926, 3308, 3314, 2399, 2408, 620... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.006697\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 39\n",
      "22:33:35.037047 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.037104 line        27     def __getitem__(self, idx):\n",
      "22:33:35.037137 line        31         else:\n",
      "New var:....... text_a = '内科医生推荐:冬天这样吃，3个月治好15年的老胃病，珍藏'\n",
      "New var:....... text_b = '睡前吃一个，48小时杀死幽门螺杆菌，不出一周治好老胃病'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.037530 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.037607 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.037680 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.037937 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['内', '科', '医', '生', '推', '荐', ':', '冬', '天', '这...好', '15', '年', '的', '老', '胃', '病', '，', '珍', '藏']\n",
      "22:33:35.038711 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.038999 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '内', '科', '医', '生', '推', '荐', ':', '冬'... '年', '的', '老', '胃', '病', '，', '珍', '藏', '[SEP]']\n",
      "22:33:35.039155 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 29\n",
      "22:33:35.039317 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['睡', '前', '吃', '一', '个', '，', '48', '小', '时', '...'，', '不', '出', '一', '周', '治', '好', '老', '胃', '病']\n",
      "22:33:35.039940 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '内', '科', '医', '生', '推', '荐', ':', '冬'... '出', '一', '周', '治', '好', '老', '胃', '病', '[SEP]']\n",
      "22:33:35.040132 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 27\n",
      "22:33:35.040401 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1079, 4906, 1278, 4495, 2972, 5773, 131, 1...39, 671, 1453, 3780, 1962, 5439, 5517, 4567, 102]\n",
      "22:33:35.040758 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1079, 4906, 1278, 4495, 2972, 5773...  671, 1453, 3780, 1962, 5439, 5517, 4567,  102])\n",
      "22:33:35.040935 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.041707 line        54 \n",
      "22:33:35.042891 return      54 \n",
      "Return value:.. (tensor([ 101, 1079, 4906, 1278, 4495, 2972, 577...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008105\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 40\n",
      "22:33:35.045182 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.045247 line        27     def __getitem__(self, idx):\n",
      "22:33:35.045284 line        31         else:\n",
      "New var:....... text_a = '王者荣耀年底停止运营？天美听了想打人！'\n",
      "New var:....... text_b = '18号下午开放系统验证？纯属谣言，苹果已经回应'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.045709 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.045872 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.045941 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.046180 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['王', '者', '荣', '耀', '年', '底', '停', '止', '运', '营', '？', '天', '美', '听', '了', '想', '打', '人', '！']\n",
      "22:33:35.046736 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.046963 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '者', '荣', '耀', '年', '底', '停', '止'... '天', '美', '听', '了', '想', '打', '人', '！', '[SEP]']\n",
      "22:33:35.047199 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 21\n",
      "22:33:35.047378 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['18', '号', '下', '午', '开', '放', '系', '统', '验', '...'属', '谣', '言', '，', '苹', '果', '已', '经', '回', '应']\n",
      "22:33:35.047997 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '者', '荣', '耀', '年', '底', '停', '止'... '言', '，', '苹', '果', '已', '经', '回', '应', '[SEP]']\n",
      "22:33:35.048187 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 23\n",
      "22:33:35.048369 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 4374, 5442, 5783, 5438, 2399, 2419, 977, 3...1, 8024, 5741, 3362, 2347, 5307, 1726, 2418, 102]\n",
      "22:33:35.048747 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4374, 5442, 5783, 5438, 2399, 2419... 8024, 5741, 3362, 2347, 5307, 1726, 2418,  102])\n",
      "22:33:35.048948 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.049521 line        54 \n",
      "22:33:35.050535 return      54 \n",
      "Return value:.. (tensor([ 101, 4374, 5442, 5783, 5438, 2399, 241... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007328\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 41\n",
      "22:33:35.052535 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.052592 line        27     def __getitem__(self, idx):\n",
      "22:33:35.052624 line        31         else:\n",
      "New var:....... text_a = '《前半生2》女主不是马伊琍，另换其人？'\n",
      "New var:....... text_b = '食物相克，这些抗癌食物不能一起吃'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.052975 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.053048 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.053104 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.053265 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['《', '前', '半', '生', '2', '》', '女', '主', '不', '是', '马', '伊', '琍', '，', '另', '换', '其', '人', '？']\n",
      "22:33:35.053992 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.054263 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '前', '半', '生', '2', '》', '女', '主'... '伊', '琍', '，', '另', '换', '其', '人', '？', '[SEP]']\n",
      "22:33:35.054417 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 21\n",
      "22:33:35.054576 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['食', '物', '相', '克', '，', '这', '些', '抗', '癌', '食', '物', '不', '能', '一', '起', '吃']\n",
      "22:33:35.055020 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '前', '半', '生', '2', '》', '女', '主'... '癌', '食', '物', '不', '能', '一', '起', '吃', '[SEP]']\n",
      "22:33:35.055191 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 17\n",
      "22:33:35.055353 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 517, 1184, 1288, 4495, 123, 518, 1957, 712...617, 7608, 4289, 679, 5543, 671, 6629, 1391, 102]\n",
      "22:33:35.055664 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 1184, 1288, 4495,  123,  518...4289,  679, 5543,  671, 6629,        1391,  102])\n",
      "22:33:35.055848 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.056395 line        54 \n",
      "22:33:35.057335 return      54 \n",
      "Return value:.. (tensor([ 101,  517, 1184, 1288, 4495,  123,  51... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.006467\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 42\n",
      "22:33:35.059025 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.059081 line        27     def __getitem__(self, idx):\n",
      "22:33:35.059113 line        31         else:\n",
      "New var:....... text_a = '有人说空心菜是蔬菜里的“毒中之王”，还能放心吃吗？'\n",
      "New var:....... text_b = '每天8杯水、散步最养生…十大常见养生谣言，信了就是害自己'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.059442 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.059515 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.059570 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.059730 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['有', '人', '说', '空', '心', '菜', '是', '蔬', '菜', '里... '[UNK]', '，', '还', '能', '放', '心', '吃', '吗', '？']\n",
      "22:33:35.060359 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.060619 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '有', '人', '说', '空', '心', '菜', '是', '蔬'... '，', '还', '能', '放', '心', '吃', '吗', '？', '[SEP]']\n",
      "22:33:35.060770 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 27\n",
      "22:33:35.060927 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['每', '天', '8', '杯', '水', '、', '散', '步', '最', '养...'谣', '言', '，', '信', '了', '就', '是', '害', '自', '己']\n",
      "22:33:35.061535 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '有', '人', '说', '空', '心', '菜', '是', '蔬'... '，', '信', '了', '就', '是', '害', '自', '己', '[SEP]']\n",
      "22:33:35.061708 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 29\n",
      "22:33:35.061935 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3300, 782, 6432, 4958, 2552, 5831, 3221, 5...024, 928, 749, 2218, 3221, 2154, 5632, 2346, 102]\n",
      "22:33:35.062227 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3300,  782, 6432, 4958, 2552, 5831...  928,  749, 2218, 3221, 2154, 5632, 2346,  102])\n",
      "22:33:35.062414 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.063055 line        54 \n",
      "22:33:35.064252 return      54 \n",
      "Return value:.. (tensor([ 101, 3300,  782, 6432, 4958, 2552, 583...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007363\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 43\n",
      "22:33:35.066412 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.066468 line        27     def __getitem__(self, idx):\n",
      "22:33:35.066499 line        31         else:\n",
      "New var:....... text_a = '“飞机就要起飞，一个男人在机舱口跪下！”'\n",
      "New var:....... text_b = '飞机就要起飞，一个男人在机舱口跪下！这一幕让人泪崩'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.067008 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.067081 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.067136 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.067310 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['[UNK]', '飞', '机', '就', '要', '起', '飞', '，', '一'... '人', '在', '机', '舱', '口', '跪', '下', '！', '[UNK]']\n",
      "22:33:35.067807 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.068068 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '[UNK]', '飞', '机', '就', '要', '起', '飞',..., '机', '舱', '口', '跪', '下', '！', '[UNK]', '[SEP]']\n",
      "22:33:35.068219 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 22\n",
      "22:33:35.068417 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['飞', '机', '就', '要', '起', '飞', '，', '一', '个', '男...'跪', '下', '！', '这', '一', '幕', '让', '人', '泪', '崩']\n",
      "22:33:35.069187 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '[UNK]', '飞', '机', '就', '要', '起', '飞',... '！', '这', '一', '幕', '让', '人', '泪', '崩', '[SEP]']\n",
      "22:33:35.069427 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 26\n",
      "22:33:35.069596 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 100, 7607, 3322, 2218, 6206, 6629, 7607, 8...013, 6821, 671, 2391, 6375, 782, 3801, 2309, 102]\n",
      "22:33:35.069869 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  100, 7607, 3322, 2218, 6206, 6629... 6821,  671, 2391, 6375,  782, 3801, 2309,  102])\n",
      "22:33:35.070054 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.070654 line        54 \n",
      "22:33:35.071717 return      54 \n",
      "Return value:.. (tensor([ 101,  100, 7607, 3322, 2218, 6206, 662... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007135\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 44\n",
      "22:33:35.073571 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.073626 line        27     def __getitem__(self, idx):\n",
      "22:33:35.073657 line        31         else:\n",
      "New var:....... text_a = '日本311大地震后7年 福岛玫瑰园如今杂草丛生'\n",
      "New var:....... text_b = '辟谣：福岛核电站影响日本留学吗'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.073975 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.074049 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.074104 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.074317 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['日', '本', '311', '大', '地', '震', '后', '7', '年', ...'岛', '玫', '瑰', '园', '如', '今', '杂', '草', '丛', '生']\n",
      "22:33:35.074829 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.075088 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '日', '本', '311', '大', '地', '震', '后', '... '瑰', '园', '如', '今', '杂', '草', '丛', '生', '[SEP]']\n",
      "22:33:35.075241 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 22\n",
      "22:33:35.075398 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['辟', '谣', '：', '福', '岛', '核', '电', '站', '影', '响', '日', '本', '留', '学', '吗']\n",
      "22:33:35.075826 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '日', '本', '311', '大', '地', '震', '后', '... '站', '影', '响', '日', '本', '留', '学', '吗', '[SEP]']\n",
      "22:33:35.075998 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 16\n",
      "22:33:35.076161 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3189, 3315, 10256, 1920, 1765, 7448, 1400,...1, 2512, 1510, 3189, 3315, 4522, 2110, 1408, 102]\n",
      "22:33:35.076469 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([  101,  3189,  3315, 10256,  1920,  1765... 1510,  3189,  3315,  4522,  2110,  1408,   102])\n",
      "22:33:35.076653 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.077195 line        54 \n",
      "22:33:35.078147 return      54 \n",
      "Return value:.. (tensor([  101,  3189,  3315, 10256,  1920,  176... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.006319\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 45\n",
      "22:33:35.079914 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.079991 line        27     def __getitem__(self, idx):\n",
      "22:33:35.080037 line        31         else:\n",
      "New var:....... text_a = '全面取消中考？美术纳入高考？不属实！'\n",
      "New var:....... text_b = '教育部辟谣科学美术纳入中高考：未发布过该消息'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.080398 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.080471 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.080526 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.080763 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['全', '面', '取', '消', '中', '考', '？', '美', '术', '纳', '入', '高', '考', '？', '不', '属', '实', '！']\n",
      "22:33:35.081247 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.081504 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '全', '面', '取', '消', '中', '考', '？', '美'... '入', '高', '考', '？', '不', '属', '实', '！', '[SEP]']\n",
      "22:33:35.081653 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 20\n",
      "22:33:35.081808 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['教', '育', '部', '辟', '谣', '科', '学', '美', '术', '纳...'高', '考', '：', '未', '发', '布', '过', '该', '消', '息']\n",
      "22:33:35.082330 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '全', '面', '取', '消', '中', '考', '？', '美'... '：', '未', '发', '布', '过', '该', '消', '息', '[SEP]']\n",
      "22:33:35.082498 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 23\n",
      "22:33:35.082661 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1059, 7481, 1357, 3867, 704, 5440, 8043, 5...8, 3313, 1355, 2357, 6814, 6421, 3867, 2622, 102]\n",
      "22:33:35.083054 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1059, 7481, 1357, 3867,  704, 5440...       1355, 2357, 6814, 6421, 3867, 2622,  102])\n",
      "22:33:35.083262 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.083915 line        54 \n",
      "22:33:35.084605 return      54 \n",
      "Return value:.. (tensor([ 101, 1059, 7481, 1357, 3867,  704, 544... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.006763\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 46\n",
      "22:33:35.086703 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.086761 line        27     def __getitem__(self, idx):\n",
      "22:33:35.086794 line        31         else:\n",
      "New var:....... text_a = '交通违法“清零”行动开始了！'\n",
      "New var:....... text_b = '辟谣丨听说交通违法“清零行动”卷土重来？'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.087118 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.087194 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.087251 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.087468 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['交', '通', '违', '法', '[UNK]', '清', '零', '[UNK]', '行', '动', '开', '始', '了', '！']\n",
      "22:33:35.087885 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.088143 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '交', '通', '违', '法', '[UNK]', '清', '零', '[UNK]', '行', '动', '开', '始', '了', '！', '[SEP]']\n",
      "22:33:35.088294 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 16\n",
      "22:33:35.088450 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['辟', '谣', '丨', '听', '说', '交', '通', '违', '法', '[... '零', '行', '动', '[UNK]', '卷', '土', '重', '来', '？']\n",
      "22:33:35.089005 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '交', '通', '违', '法', '[UNK]', '清', '零',..., '动', '[UNK]', '卷', '土', '重', '来', '？', '[SEP]']\n",
      "22:33:35.089176 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 21\n",
      "22:33:35.089407 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 769, 6858, 6824, 3791, 100, 3926, 7439, 10...21, 1220, 100, 1318, 1759, 7028, 3341, 8043, 102]\n",
      "22:33:35.089657 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  769, 6858, 6824, 3791,  100, 3926... 100, 1318, 1759, 7028, 3341, 8043,         102])\n",
      "22:33:35.089841 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.090381 line        54 \n",
      "22:33:35.091438 return      54 \n",
      "Return value:.. (tensor([ 101,  769, 6858, 6824, 3791,  100, 392... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.006464\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 47\n",
      "22:33:35.093191 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.093248 line        27     def __getitem__(self, idx):\n",
      "22:33:35.093280 line        31         else:\n",
      "New var:....... text_a = '吃了荔枝，就会查出酒驾？交警的实验告诉你这是真的！'\n",
      "New var:....... text_b = '开车别吃荔枝\\xa0会变成“醉驾”的'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.093600 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.093675 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.093731 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.093901 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['吃', '了', '荔', '枝', '，', '就', '会', '查', '出', '酒...'实', '验', '告', '诉', '你', '这', '是', '真', '的', '！']\n",
      "22:33:35.094467 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.094729 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '吃', '了', '荔', '枝', '，', '就', '会', '查'... '告', '诉', '你', '这', '是', '真', '的', '！', '[SEP]']\n",
      "22:33:35.094883 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 27\n",
      "22:33:35.095042 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['开', '车', '别', '吃', '荔', '枝', '会', '变', '成', '[UNK]', '醉', '驾', '[UNK]', '的']\n",
      "22:33:35.095466 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '吃', '了', '荔', '枝', '，', '就', '会', '查'...', '成', '[UNK]', '醉', '驾', '[UNK]', '的', '[SEP]']\n",
      "22:33:35.096168 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 15\n",
      "22:33:35.096306 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1391, 749, 5775, 3363, 8024, 2218, 833, 33...833, 1359, 2768, 100, 7004, 7730, 100, 4638, 102]\n",
      "22:33:35.096596 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1391,  749, 5775, 3363, 8024, 2218...2768,         100, 7004, 7730,  100, 4638,  102])\n",
      "22:33:35.096782 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.097349 line        54 \n",
      "22:33:35.098773 return      54 \n",
      "Return value:.. (tensor([ 101, 1391,  749, 5775, 3363, 8024, 221... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008639\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 48\n",
      "22:33:35.101859 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.102074 line        27     def __getitem__(self, idx):\n",
      "22:33:35.102110 line        31         else:\n",
      "New var:....... text_a = '喝酒不醉的技巧竟是这？绝对意想不到的千杯不醉小妙招'\n",
      "New var:....... text_b = '喝酒千杯不醉的小妙招，你们学会了吗？'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.102511 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.102588 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.102650 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.102903 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['喝', '酒', '不', '醉', '的', '技', '巧', '竟', '是', '这...'不', '到', '的', '千', '杯', '不', '醉', '小', '妙', '招']\n",
      "22:33:35.103580 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.104034 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '喝', '酒', '不', '醉', '的', '技', '巧', '竟'... '的', '千', '杯', '不', '醉', '小', '妙', '招', '[SEP]']\n",
      "22:33:35.104295 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 27\n",
      "22:33:35.104557 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['喝', '酒', '千', '杯', '不', '醉', '的', '小', '妙', '招', '，', '你', '们', '学', '会', '了', '吗', '？']\n",
      "22:33:35.105162 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '喝', '酒', '不', '醉', '的', '技', '巧', '竟'... '，', '你', '们', '学', '会', '了', '吗', '？', '[SEP]']\n",
      "22:33:35.105570 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 19\n",
      "22:33:35.105826 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1600, 6983, 679, 7004, 4638, 2825, 2341, 4... 8024, 872, 812, 2110, 833, 749, 1408, 8043, 102]\n",
      "22:33:35.106221 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1600, 6983,  679, 7004, 4638, 2825...  872,  812, 2110,  833,  749, 1408, 8043,  102])\n",
      "22:33:35.106496 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.108020 line        54 \n",
      "22:33:35.109615 return      54 \n",
      "Return value:.. (tensor([ 101, 1600, 6983,  679, 7004, 4638, 282... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010672\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 49\n",
      "22:33:35.112560 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.112618 line        27     def __getitem__(self, idx):\n",
      "22:33:35.112651 line        31         else:\n",
      "New var:....... text_a = '手机上免费看直播，世界上所有频道都能观看，打开微信就可以'\n",
      "New var:....... text_b = '不用再充VIP，教你一招，用微信可免费观看全网VIP视频'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.113064 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.113161 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.113238 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.113493 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['手', '机', '上', '免', '费', '看', '直', '播', '，', '世...'观', '看', '，', '打', '开', '微', '信', '就', '可', '以']\n",
      "22:33:35.114343 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.114633 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '手', '机', '上', '免', '费', '看', '直', '播'... '，', '打', '开', '微', '信', '就', '可', '以', '[SEP]']\n",
      "22:33:35.114871 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 30\n",
      "22:33:35.115263 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['不', '用', '再', '充', '[UNK]', '，', '教', '你', '一'... '免', '费', '观', '看', '全', '网', '[UNK]', '视', '频']\n",
      "22:33:35.115934 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '手', '机', '上', '免', '费', '看', '直', '播'..., '观', '看', '全', '网', '[UNK]', '视', '频', '[SEP]']\n",
      "22:33:35.116196 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 25\n",
      "22:33:35.116597 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 2797, 3322, 677, 1048, 6589, 4692, 4684, 3...89, 6225, 4692, 1059, 5381, 100, 6228, 7574, 102]\n",
      "22:33:35.116977 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2797, 3322,  677, 1048, 6589, 4692...       4692, 1059, 5381,  100, 6228, 7574,  102])\n",
      "22:33:35.117266 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.118493 line        54 \n",
      "22:33:35.120532 return      54 \n",
      "Return value:.. (tensor([ 101, 2797, 3322,  677, 1048, 6589, 469...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.009253\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 50\n",
      "22:33:35.121839 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.121897 line        27     def __getitem__(self, idx):\n",
      "22:33:35.121930 line        31         else:\n",
      "New var:....... text_a = '于文华母亲去世，葬礼上这首歌让于文华痛哭流涕，全村都难过落泪'\n",
      "New var:....... text_b = '于文华母亲去世，葬礼上这首歌让于文华痛不欲生，全村都难过落泪'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.122281 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.122357 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.122414 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.122590 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['于', '文', '华', '母', '亲', '去', '世', '，', '葬', '礼...'流', '涕', '，', '全', '村', '都', '难', '过', '落', '泪']\n",
      "22:33:35.123223 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.123491 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '于', '文', '华', '母', '亲', '去', '世', '，'... '，', '全', '村', '都', '难', '过', '落', '泪', '[SEP]']\n",
      "22:33:35.123647 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 32\n",
      "22:33:35.123808 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['于', '文', '华', '母', '亲', '去', '世', '，', '葬', '礼...'欲', '生', '，', '全', '村', '都', '难', '过', '落', '泪']\n",
      "22:33:35.124420 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "22:33:35.124574 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 31\n",
      "22:33:35.124793 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 754, 3152, 1290, 3678, 779, 1343, 686, 802...4, 1059, 3333, 6963, 7410, 6814, 5862, 3801, 102]\n",
      "22:33:35.125105 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  754, 3152, 1290, 3678,  779, 1343...3333, 6963, 7410, 6814,        5862, 3801,  102])\n",
      "22:33:35.125303 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.125995 line        54 \n",
      "22:33:35.127388 return      54 \n",
      "Return value:.. (tensor([ 101,  754, 3152, 1290, 3678,  779, 134... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008225\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 51\n",
      "22:33:35.130089 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.130147 line        27     def __getitem__(self, idx):\n",
      "22:33:35.130179 line        31         else:\n",
      "New var:....... text_a = '好尴尬 穿中国风百蝶衣亮相戛纳红毯，被保安驱赶了三次'\n",
      "New var:....... text_b = '李玉刚工作室发声明 澄清戛纳电影节红毯“被驱赶”谣言'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.130578 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.130656 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.130713 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.130937 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['好', '尴', '尬', '穿', '中', '国', '风', '百', '蝶', '衣...'毯', '，', '被', '保', '安', '驱', '赶', '了', '三', '次']\n",
      "22:33:35.131506 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.131770 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '好', '尴', '尬', '穿', '中', '国', '风', '百'... '被', '保', '安', '驱', '赶', '了', '三', '次', '[SEP]']\n",
      "22:33:35.131924 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 27\n",
      "22:33:35.132083 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['李', '玉', '刚', '工', '作', '室', '发', '声', '明', '澄..., '毯', '[UNK]', '被', '驱', '赶', '[UNK]', '谣', '言']\n",
      "22:33:35.132659 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '好', '尴', '尬', '穿', '中', '国', '风', '百'...UNK]', '被', '驱', '赶', '[UNK]', '谣', '言', '[SEP]']\n",
      "22:33:35.132844 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 26\n",
      "22:33:35.133262 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1962, 2219, 2217, 4959, 704, 1744, 7599, 4...691, 100, 6158, 7721, 6628, 100, 6469, 6241, 102]\n",
      "22:33:35.133676 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1962, 2219, 2217, 4959,  704, 1744...6158, 7721,        6628,  100, 6469, 6241,  102])\n",
      "22:33:35.133872 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "22:33:35.134583 line        54 \n",
      "22:33:35.136312 return      54 \n",
      "Return value:.. (tensor([ 101, 1962, 2219, 2217, 4959,  704, 174...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008281\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 52\n",
      "22:33:35.138449 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.138507 line        27     def __getitem__(self, idx):\n",
      "22:33:35.138539 line        31         else:\n",
      "New var:....... text_a = '九江市民疑在九九隆超市买到“塑料紫菜”，打汤后嚼不烂难扯断'\n",
      "New var:....... text_b = '龙口粉丝是塑料做的吗？鲁中网记者现场实验击破谣言'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.139370 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.139445 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.139528 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.140080 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['九', '江', '市', '民', '疑', '在', '九', '九', '隆', '超...'，', '打', '汤', '后', '嚼', '不', '烂', '难', '扯', '断']\n",
      "22:33:35.140815 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.141028 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '九', '江', '市', '民', '疑', '在', '九', '九'... '汤', '后', '嚼', '不', '烂', '难', '扯', '断', '[SEP]']\n",
      "22:33:35.141255 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 31\n",
      "22:33:35.141483 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['龙', '口', '粉', '丝', '是', '塑', '料', '做', '的', '吗...'记', '者', '现', '场', '实', '验', '击', '破', '谣', '言']\n",
      "22:33:35.142139 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '九', '江', '市', '民', '疑', '在', '九', '九'... '现', '场', '实', '验', '击', '破', '谣', '言', '[SEP]']\n",
      "22:33:35.142318 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 25\n",
      "22:33:35.142495 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 736, 3736, 2356, 3696, 4542, 1762, 736, 73...5, 1767, 2141, 7741, 1140, 4788, 6469, 6241, 102]\n",
      "22:33:35.142827 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  736, 3736, 2356, 3696, 4542, 1762... 1767, 2141, 7741, 1140, 4788, 6469, 6241,  102])\n",
      "22:33:35.143135 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.143998 line        54 \n",
      "22:33:35.145424 return      54 \n",
      "Return value:.. (tensor([ 101,  736, 3736, 2356, 3696, 4542, 176...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.009117\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 53\n",
      "22:33:35.147594 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.147653 line        27     def __getitem__(self, idx):\n",
      "22:33:35.147685 line        31         else:\n",
      "New var:....... text_a = '腰突俩把野生艾草轻松搞定，3天去根不复发，腰腿疼也管用'\n",
      "New var:....... text_b = '腰间盘突出危害大！一把艾草帮你忙，颈椎病也有效果！'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.148070 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.148144 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.148201 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.148429 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['腰', '突', '俩', '把', '野', '生', '艾', '草', '轻', '松...'不', '复', '发', '，', '腰', '腿', '疼', '也', '管', '用']\n",
      "22:33:35.149025 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.149387 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '腰', '突', '俩', '把', '野', '生', '艾', '草'... '发', '，', '腰', '腿', '疼', '也', '管', '用', '[SEP]']\n",
      "22:33:35.149681 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 29\n",
      "22:33:35.149895 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['腰', '间', '盘', '突', '出', '危', '害', '大', '！', '一...'忙', '，', '颈', '椎', '病', '也', '有', '效', '果', '！']\n",
      "22:33:35.150583 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '腰', '突', '俩', '把', '野', '生', '艾', '草'... '颈', '椎', '病', '也', '有', '效', '果', '！', '[SEP]']\n",
      "22:33:35.150908 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 26\n",
      "22:33:35.151102 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 5587, 4960, 930, 2828, 7029, 4495, 5687, 5...68, 3491, 4567, 738, 3300, 3126, 3362, 8013, 102]\n",
      "22:33:35.151606 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5587, 4960,  930, 2828, 7029, 4495...       4567,  738, 3300, 3126, 3362, 8013,  102])\n",
      "22:33:35.151766 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.152705 line        54 \n",
      "22:33:35.154022 return      54 \n",
      "Return value:.. (tensor([ 101, 5587, 4960,  930, 2828, 7029, 449...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008810\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 54\n",
      "22:33:35.156431 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.156495 line        27     def __getitem__(self, idx):\n",
      "22:33:35.156646 line        31         else:\n",
      "New var:....... text_a = '男人这样吃鸡蛋，金枪不倒半小时，吃三天就见效！'\n",
      "New var:....... text_b = '男人吃大蒜强精健身？'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.157164 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.157240 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.157363 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.157676 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['男', '人', '这', '样', '吃', '鸡', '蛋', '，', '金', '枪...'小', '时', '，', '吃', '三', '天', '就', '见', '效', '！']\n",
      "22:33:35.158472 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.158891 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '男', '人', '这', '样', '吃', '鸡', '蛋', '，'... '，', '吃', '三', '天', '就', '见', '效', '！', '[SEP]']\n",
      "22:33:35.159122 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 25\n",
      "22:33:35.159314 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['男', '人', '吃', '大', '蒜', '强', '精', '健', '身', '？']\n",
      "22:33:35.159736 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '男', '人', '这', '样', '吃', '鸡', '蛋', '，'... '吃', '大', '蒜', '强', '精', '健', '身', '？', '[SEP]']\n",
      "22:33:35.160247 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 11\n",
      "22:33:35.160774 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 4511, 782, 6821, 3416, 1391, 7883, 6028, 8...91, 1920, 5886, 2487, 5125, 978, 6716, 8043, 102]\n",
      "22:33:35.161146 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4511,  782, 6821, 3416, 1391, 7883... 1920, 5886, 2487, 5125,  978, 6716, 8043,  102])\n",
      "22:33:35.161366 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0,        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.161958 line        54 \n",
      "22:33:35.162960 return      54 \n",
      "Return value:.. (tensor([ 101, 4511,  782, 6821, 3416, 1391, 788... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008333\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 55\n",
      "22:33:35.164792 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.164849 line        27     def __getitem__(self, idx):\n",
      "22:33:35.164882 line        31         else:\n",
      "New var:....... text_a = '无核葡萄用避孕药，吃了会绝育？！'\n",
      "New var:....... text_b = '棉花肉松是谣言，但有不含肉的假肉松'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.165292 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.165445 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.165504 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.165668 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['无', '核', '葡', '萄', '用', '避', '孕', '药', '，', '吃', '了', '会', '绝', '育', '？', '！']\n",
      "22:33:35.166127 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.166378 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '无', '核', '葡', '萄', '用', '避', '孕', '药', '，', '吃', '了', '会', '绝', '育', '？', '！', '[SEP]']\n",
      "22:33:35.166610 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 18\n",
      "22:33:35.167275 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['棉', '花', '肉', '松', '是', '谣', '言', '，', '但', '有', '不', '含', '肉', '的', '假', '肉', '松']\n",
      "22:33:35.167997 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '无', '核', '葡', '萄', '用', '避', '孕', '药'... '有', '不', '含', '肉', '的', '假', '肉', '松', '[SEP]']\n",
      "22:33:35.168323 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 18\n",
      "22:33:35.168541 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 3187, 3417, 5868, 5843, 4500, 6912, 2097, ...300, 679, 1419, 5489, 4638, 969, 5489, 3351, 102]\n",
      "22:33:35.168888 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3187, 3417, 5868, 5843, 4500, 6912...  679, 1419, 5489, 4638,  969, 5489, 3351,  102])\n",
      "22:33:35.169119 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.169980 line        54 \n",
      "22:33:35.171065 return      54 \n",
      "Return value:.. (tensor([ 101, 3187, 3417, 5868, 5843, 4500, 691... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.007951\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 56\n",
      "22:33:35.172770 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.172848 line        27     def __getitem__(self, idx):\n",
      "22:33:35.172880 line        31         else:\n",
      "New var:....... text_a = '农村人注意了，这30个省农村户口要取消了'\n",
      "New var:....... text_b = '农村人注意了，这30个省农村户口要取消，不知道你家乡有吗？'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.173319 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.173489 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.173562 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.173805 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['农', '村', '人', '注', '意', '了', '，', '这', '30', '个', '省', '农', '村', '户', '口', '要', '取', '消', '了']\n",
      "22:33:35.174405 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.174779 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '农', '村', '人', '注', '意', '了', '，', '这'... '农', '村', '户', '口', '要', '取', '消', '了', '[SEP]']\n",
      "22:33:35.174955 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 21\n",
      "22:33:35.175135 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['农', '村', '人', '注', '意', '了', '，', '这', '30', '...'，', '不', '知', '道', '你', '家', '乡', '有', '吗', '？']\n",
      "22:33:35.175881 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '农', '村', '人', '注', '意', '了', '，', '这'... '知', '道', '你', '家', '乡', '有', '吗', '？', '[SEP]']\n",
      "22:33:35.176168 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 29\n",
      "22:33:35.176372 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1093, 3333, 782, 3800, 2692, 749, 8024, 68...761, 6887, 872, 2157, 740, 3300, 1408, 8043, 102]\n",
      "22:33:35.176713 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1093, 3333,  782, 3800, 2692,  749... 872, 2157,  740, 3300, 1408,        8043,  102])\n",
      "22:33:35.176964 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "22:33:35.177708 line        54 \n",
      "22:33:35.178756 return      54 \n",
      "Return value:.. (tensor([ 101, 1093, 3333,  782, 3800, 2692,  74...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008131\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 57\n",
      "22:33:35.180929 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.180991 line        27     def __getitem__(self, idx):\n",
      "22:33:35.181025 line        31         else:\n",
      "New var:....... text_a = '小米Note系列新机曝光，骁龙835+5.99寸全面屏2.0'\n",
      "New var:....... text_b = '诺基亚新旗舰曝光，骁龙835+6GB媲美小米6吗？'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.181458 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.181628 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.181703 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.181983 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['小', '米', '[UNK]', '系', '列', '新', '机', '曝', '光'...5', '.', '99', '寸', '全', '面', '屏', '2', '.', '0']\n",
      "22:33:35.182625 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.182909 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '米', '[UNK]', '系', '列', '新', '机',...'99', '寸', '全', '面', '屏', '2', '.', '0', '[SEP]']\n",
      "22:33:35.183069 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 26\n",
      "22:33:35.183238 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['诺', '基', '亚', '新', '旗', '舰', '曝', '光', '，', '骁... '+', '[UNK]', '媲', '美', '小', '米', '6', '吗', '？']\n",
      "22:33:35.183907 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '米', '[UNK]', '系', '列', '新', '机',...NK]', '媲', '美', '小', '米', '6', '吗', '？', '[SEP]']\n",
      "22:33:35.184244 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 22\n",
      "22:33:35.184422 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 2207, 5101, 100, 5143, 1154, 3173, 3322, 3...00, 2059, 5401, 2207, 5101, 127, 1408, 8043, 102]\n",
      "22:33:35.184912 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([  101,  2207,  5101,   100,  5143,  1154... 5401,  2207,  5101,   127,  1408,  8043,   102])\n",
      "22:33:35.185395 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.186167 line        54 \n",
      "22:33:35.187265 return      54 \n",
      "Return value:.. (tensor([  101,  2207,  5101,   100,  5143,  115... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008532\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 58\n",
      "22:33:35.189488 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.189549 line        27     def __getitem__(self, idx):\n",
      "22:33:35.189583 line        31         else:\n",
      "New var:....... text_a = '央视公布17种“巨毒食品”？实为恶意篡改编造和旧闻翻炒'\n",
      "New var:....... text_b = '燕窝中含有激素？你被这个谣言骗了吧！'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.189951 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.190101 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.190162 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.190325 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['央', '视', '公', '布', '17', '种', '[UNK]', '巨', '毒...'意', '篡', '改', '编', '造', '和', '旧', '闻', '翻', '炒']\n",
      "22:33:35.190952 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.191171 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '央', '视', '公', '布', '17', '种', '[UNK]'... '改', '编', '造', '和', '旧', '闻', '翻', '炒', '[SEP]']\n",
      "22:33:35.191429 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 28\n",
      "22:33:35.191621 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['燕', '窝', '中', '含', '有', '激', '素', '？', '你', '被', '这', '个', '谣', '言', '骗', '了', '吧', '！']\n",
      "22:33:35.192267 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '央', '视', '公', '布', '17', '种', '[UNK]'... '这', '个', '谣', '言', '骗', '了', '吧', '！', '[SEP]']\n",
      "22:33:35.192473 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 19\n",
      "22:33:35.192987 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1925, 6228, 1062, 2357, 8126, 4905, 100, 2...821, 702, 6469, 6241, 7745, 749, 1416, 8013, 102]\n",
      "22:33:35.193422 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1925, 6228, 1062, 2357, 8126, 4905...  702, 6469, 6241, 7745,  749, 1416, 8013,  102])\n",
      "22:33:35.193639 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.194429 line        54 \n",
      "22:33:35.195540 return      54 \n",
      "Return value:.. (tensor([ 101, 1925, 6228, 1062, 2357, 8126, 490... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.008154\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 59\n",
      "22:33:35.197667 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.197723 line        27     def __getitem__(self, idx):\n",
      "22:33:35.197755 line        31         else:\n",
      "New var:....... text_a = '农村四项费用全面取消，别不知道白交了'\n",
      "New var:....... text_b = '现在在农村这几项费用交了等于白交，国家都取消了'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.198129 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.198275 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.198334 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.198512 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['农', '村', '四', '项', '费', '用', '全', '面', '取', '消', '，', '别', '不', '知', '道', '白', '交', '了']\n",
      "22:33:35.199024 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.199225 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '农', '村', '四', '项', '费', '用', '全', '面'... '，', '别', '不', '知', '道', '白', '交', '了', '[SEP]']\n",
      "22:33:35.199578 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 20\n",
      "22:33:35.199789 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['现', '在', '在', '农', '村', '这', '几', '项', '费', '用...'于', '白', '交', '，', '国', '家', '都', '取', '消', '了']\n",
      "22:33:35.200633 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '农', '村', '四', '项', '费', '用', '全', '面'... '交', '，', '国', '家', '都', '取', '消', '了', '[SEP]']\n",
      "22:33:35.201040 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 24\n",
      "22:33:35.201220 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1093, 3333, 1724, 7555, 6589, 4500, 1059, ...69, 8024, 1744, 2157, 6963, 1357, 3867, 749, 102]\n",
      "22:33:35.201496 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1093, 3333, 1724, 7555, 6589, 4500... 8024, 1744, 2157, 6963, 1357, 3867,  749,  102])\n",
      "22:33:35.201687 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.202237 line        54 \n",
      "22:33:35.203495 return      54 \n",
      "Return value:.. (tensor([ 101, 1093, 3333, 1724, 7555, 6589, 450... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007742\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 60\n",
      "22:33:35.205441 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.205499 line        27     def __getitem__(self, idx):\n",
      "22:33:35.205531 line        31         else:\n",
      "New var:....... text_a = '5月1日起全国公交成失联儿童守护点？真假？'\n",
      "New var:....... text_b = '5月1日起，全国公交正式成为中国失联儿童安全守护点！'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.205951 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.206027 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.206157 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.206317 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['5', '月', '1', '日', '起', '全', '国', '公', '交', '成...'联', '儿', '童', '守', '护', '点', '？', '真', '假', '？']\n",
      "22:33:35.206844 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.207113 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '5', '月', '1', '日', '起', '全', '国', '公'... '童', '守', '护', '点', '？', '真', '假', '？', '[SEP]']\n",
      "22:33:35.207268 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 23\n",
      "22:33:35.207427 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['5', '月', '1', '日', '起', '，', '全', '国', '公', '交...'失', '联', '儿', '童', '安', '全', '守', '护', '点', '！']\n",
      "22:33:35.208308 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '5', '月', '1', '日', '起', '全', '国', '公'... '儿', '童', '安', '全', '守', '护', '点', '！', '[SEP]']\n",
      "22:33:35.208602 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 27\n",
      "22:33:35.208905 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 126, 3299, 122, 3189, 6629, 1059, 1744, 10...6, 4997, 2128, 1059, 2127, 2844, 4157, 8013, 102]\n",
      "22:33:35.209431 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  126, 3299,  122, 3189, 6629, 1059...2128, 1059, 2127, 2844, 4157,        8013,  102])\n",
      "22:33:35.209701 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "22:33:35.210503 line        54 \n",
      "22:33:35.211922 return      54 \n",
      "Return value:.. (tensor([ 101,  126, 3299,  122, 3189, 6629, 105...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009903\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 61\n",
      "22:33:35.215418 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.215538 line        27     def __getitem__(self, idx):\n",
      "22:33:35.215616 line        31         else:\n",
      "New var:....... text_a = '一家三口吃“黑美人”西瓜死亡'\n",
      "New var:....... text_b = '辟谣联盟“亮剑” 谣言止于智者'\n",
      "New var:....... label = 'unrelated'\n",
      "22:33:35.216443 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 2\n",
      "22:33:35.216769 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(2)\n",
      "22:33:35.217008 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.217547 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['一', '家', '三', '口', '吃', '[UNK]', '黑', '美', '人', '[UNK]', '西', '瓜', '死', '亡']\n",
      "22:33:35.218721 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.219083 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '家', '三', '口', '吃', '[UNK]', '黑', '美', '人', '[UNK]', '西', '瓜', '死', '亡', '[SEP]']\n",
      "22:33:35.219326 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 16\n",
      "22:33:35.219697 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['辟', '谣', '联', '盟', '[UNK]', '亮', '剑', '[UNK]', '谣', '言', '止', '于', '智', '者']\n",
      "22:33:35.220344 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '家', '三', '口', '吃', '[UNK]', '黑',..., '[UNK]', '谣', '言', '止', '于', '智', '者', '[SEP]']\n",
      "22:33:35.220674 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 15\n",
      "22:33:35.221137 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 671, 2157, 676, 1366, 1391, 100, 7946, 540...187, 100, 6469, 6241, 3632, 754, 3255, 5442, 102]\n",
      "22:33:35.221643 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101,  671, 2157,  676, 1366, 1391,  100...       6469, 6241, 3632,  754, 3255, 5442,  102])\n",
      "22:33:35.221966 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.222952 line        54 \n",
      "22:33:35.225057 return      54 \n",
      "Return value:.. (tensor([ 101,  671, 2157,  676, 1366, 1391,  10...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.012061\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 62\n",
      "22:33:35.227513 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.227581 line        27     def __getitem__(self, idx):\n",
      "22:33:35.227618 line        31         else:\n",
      "New var:....... text_a = '史上最难找的方子，为了家人留着吧，家家都有需要！'\n",
      "New var:....... text_b = '最难找的民间偏方，药到病除！为家人留着吧~'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.228209 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.228303 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.228484 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.228620 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['史', '上', '最', '难', '找', '的', '方', '子', '，', '为...'着', '吧', '，', '家', '家', '都', '有', '需', '要', '！']\n",
      "22:33:35.229325 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.229643 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '史', '上', '最', '难', '找', '的', '方', '子'... '，', '家', '家', '都', '有', '需', '要', '！', '[SEP]']\n",
      "22:33:35.229914 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 26\n",
      "22:33:35.230111 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['最', '难', '找', '的', '民', '间', '偏', '方', '，', '药...'病', '除', '！', '为', '家', '人', '留', '着', '吧', '~']\n",
      "22:33:35.230725 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '史', '上', '最', '难', '找', '的', '方', '子'... '！', '为', '家', '人', '留', '着', '吧', '~', '[SEP]']\n",
      "22:33:35.231108 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 22\n",
      "22:33:35.231316 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 1380, 677, 3297, 7410, 2823, 4638, 3175, 2...8013, 711, 2157, 782, 4522, 4708, 1416, 172, 102]\n",
      "22:33:35.231644 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1380,  677, 3297, 7410, 2823, 4638...  711, 2157,  782, 4522, 4708, 1416,  172,  102])\n",
      "22:33:35.231878 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.232596 line        54 \n",
      "22:33:35.233782 return      54 \n",
      "Return value:.. (tensor([ 101, 1380,  677, 3297, 7410, 2823, 463... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008641\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x7faeb9f5d970>\n",
      "Starting var:.. idx = 63\n",
      "22:33:35.236185 call        25     # 定义回传一些训练 / 测试数据函数\n",
      "22:33:35.236254 line        27     def __getitem__(self, idx):\n",
      "22:33:35.236292 line        31         else:\n",
      "New var:....... text_a = '开塞露可以洗脸美白？学会使用一瓶开塞露，脸就可快速变白'\n",
      "New var:....... text_b = '开塞露洗脸新用式，适合各种肤质，几天就变白，脸上光光滑滑'\n",
      "New var:....... label = 'agreed'\n",
      "22:33:35.236838 line        32             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... label_id = 0\n",
      "22:33:35.236930 line        33             label_id = self.label_map[label]\n",
      "New var:....... label_tensor = tensor(0)\n",
      "22:33:35.236999 line        36         # 建立第一个句子的 bert tokens 并加入分割符号 [SEP]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "22:33:35.237269 line        37         word_pieces = [\"[CLS]\"]\n",
      "New var:....... tokens_a = ['开', '塞', '露', '可', '以', '洗', '脸', '美', '白', '？...'塞', '露', '，', '脸', '就', '可', '快', '速', '变', '白']\n",
      "22:33:35.237977 line        38         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "22:33:35.238301 line        39         print(tokens_a)\n",
      "Modified var:.. word_pieces = ['[CLS]', '开', '塞', '露', '可', '以', '洗', '脸', '美'... '，', '脸', '就', '可', '快', '速', '变', '白', '[SEP]']\n",
      "22:33:35.238491 line        40         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "New var:....... len_a = 29\n",
      "22:33:35.238679 line        43         # 第二个句子的 BERT tokens\n",
      "New var:....... tokens_b = ['开', '塞', '露', '洗', '脸', '新', '用', '式', '，', '适...'就', '变', '白', '，', '脸', '上', '光', '光', '滑', '滑']\n",
      "22:33:35.239399 line        44         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "Modified var:.. word_pieces = ['[CLS]', '开', '塞', '露', '可', '以', '洗', '脸', '美'... '白', '，', '脸', '上', '光', '光', '滑', '滑', '[SEP]']\n",
      "22:33:35.239694 line        45         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "New var:....... len_b = 29\n",
      "22:33:35.239908 line        48         # 将整个token 序列转化成索引序列\n",
      "New var:....... ids = [101, 2458, 1853, 7463, 1377, 809, 3819, 5567, 5...35, 8024, 5567, 677, 1045, 1045, 3998, 3998, 102]\n",
      "22:33:35.240278 line        49         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2458, 1853, 7463, 1377,  809, 3819... 8024, 5567,  677, 1045, 1045, 3998, 3998,  102])\n",
      "22:33:35.240525 line        52         # 将第一句包含 [SEP] 的 token 位置设为 0, 其他为 1 标示第二句\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "22:33:35.241330 line        54 \n",
      "22:33:35.242754 return      54 \n",
      "Return value:.. (tensor([ 101, 2458, 1853, 7463, 1377,  809, 381...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['苏', '有', '朋', '要', '结', '婚', '了', '，', '但', '网', '友', '觉', '得', '他', '还', '是', '和', '林', '心', '如', '比', '较', '合', '适']\n",
      "['爆', '料', '李', '小', '璐', '要', '成', '前', '妻', '了', '贾', '乃', '亮', '模', '仿', '王', '宝', '强', '一', '步', '到', '位', '、', '快', '刀', '斩', '乱', '麻', '！']\n",
      "['为', '彩', '礼', '，', '母', '亲', '把', '女', '儿', '嫁', '给', '陌', '生', '男', '子', '，', '十', '年', '后', '再', '见', '面', '，', '母', '亲', '湿', '了', '眼', '眶']\n",
      "['猪', '油', '是', '个', '宝', '，', '一', '勺', '猪', '油', '等', '于', '十', '副', '药', '，', '先', '备', '起', '来', '再', '说']\n",
      "['剖', '析', '：', '香', '椿', '，', '为', '什', '么', '会', '致', '癌', '？']\n",
      "['你', '听', '说', '过', '[UNK]', '伪', '狂', '犬', '病', '[UNK]', '吗', '？', '真', '正', '的', '狂', '犬', '病', '人', '不', '会', '学', '狗', '叫']\n",
      "['有', '钱', '人', '买', '房', '从', '不', '选', '这', '几', '层', '？', '开', '发', '商', '不', '会', '告', '诉', '你', '的', '秘', '密', '，', '看', '完', '才', '知', '多', '坑']\n",
      "['真', '假', '？', '武', '安', '某', '商', '场', '家', '长', '因', '孩', '子', '争', '抢', '玩', '具', '大', '打', '出', '手', '致', '一', '人', '死', '亡', '？']\n",
      "['其', '他', '文', '玩', '都', '在', '暴', '跌', '！', '为', '何', '南', '红', '玛', '瑙', '价', '格', '持', '续', '上', '涨', '！']\n",
      "['「', '悦', '读', '」', '官', '方', '发', '布', '！', '恢', '复', '微', '信', '聊', '天', '记', '录', '教', '程', '！', '快', '点', '学', '起', '来', '[UNK]', '[UNK]']\n",
      "['每', '晚', '睡', '前', '洗', '脸', '后', '，', '阿', '莫', '西', '林', '配', '它', '抹', '脸', '，', '坚', '持', '4', '天', '皮', '肤', '又', '白', '又', '嫩']\n",
      "['玲', '琅', '满', '目', '的', '植', '物', '油', '做', '饭', '致', '癌', '，', '来', '告', '诉', '你', '正', '确', '的', '选', '用', '窍', '门']\n",
      "['明', '察', '｜', '大', '连', '警', '方', '：', '正', '侦', '破', '特', '大', '杀', '人', '案', '，', '网', '传', '行', '凶', '视', '频', '与', '本', '案', '无', '关']\n",
      "['一', '个', '补', '肾', '的', '偏', '方', '，', '只', '需', '几', '毛', '钱', '的', '枸', '杞', '告', '别', '时', '间', '短', '，', '一', '次', '根', '除']\n",
      "['大', '蒜', '加', '它', '一', '起', '吃', '，', '防', '血', '栓', '，', '延', '缓', '衰', '老', '，', '一', '生', '不', '得', '癌', '症', '！']\n",
      "['沙', '湾', '有', '人', '吃', '羊', '肉', '得', '了', '[UNK]', '病', '毒', '？', '切', '！', '我', '还', '[UNK]', '呢', '！']\n",
      "['林', '更', '新', '的', '一', '句', '话', '疑', '似', '支', '持', '鹿', '晗', '关', '晓', '彤', '分', '手', '，', '网', '友', '：', '也', '许', '他', '们', '会', '结', '婚', '呢']\n",
      "['夏', '季', '减', '肥', '的', '你', '千', '万', '别', '错', '过', '这', '些', '高', '效', '刮', '油', '食', '物', '哦', '！']\n",
      "['廊', '坊', '网', '传', '飞', '机', '撒', '药', '治', '白', '蛾', '市', '园', '林', '局', '：', '假', '消', '息', '！']\n",
      "['王', '珂', '再', '次', '破', '产', '欠', '六', '千', '万', '巨', '款', '！', '刘', '涛', '又', '哭', '了', '，', '夫', '妻', '一', '起', '做', '节', '目', '还', '债']\n",
      "['白', '百', '合', '电', '视', '剧', '下', '架', '，', '首', '次', '反', '驳', '出', '轨', '并', '没', '错', '！', '这', '样', '的', '女', '星', '还', '能', '理', '直', '气', '壮']\n",
      "['微', '信', '又', '出', '新', '功', '能', '了', '以', '后', '用', '微', '信', '可', '以', '免', '费', '上', '网', '了']\n",
      "['张', '一', '山', '患', '抑', '郁', '症', '将', '退', '出', '娱', '乐', '圈', '？', '回', '应', '：', '暂', '时', '死', '不', '了', '，', '拍', '完', '戏', '就', '休', '息']\n",
      "['冰', '淇', '淋', '融', '化', '的', '越', '慢', '，', '添', '加', '剂', '就', '越', '多', '吗', '？']\n",
      "['美', '国', '公', '布', '拷', '问', '抓', '到', '的', '外', '星', '人', '的', '视', '频', '，', '称', '自', '己', '来', '自', '未', '来']\n",
      "['买', '房', '万', '万', '别', '选', '这', '几', '种', '房', '子', '，', '很', '多', '人', '没', '听', '劝', '，', '交', '钱', '后', '肠', '子', '都', '悔', '青', '了']\n",
      "['付', '笛', '声', '、', '任', '静', '夫', '妻', '皈', '依', '佛', '门', '？', '一', '曲', '知', '心', '爱', '人', '重', '新', '回', '忆', '他', '们', '的', '爱', '情', '！']\n",
      "['嘲', '讽', '农', '民', '歌', '曲', '，', '那', '英', '公', '然', '放', '言', '刀', '郎', '砸', '电', '视', '！']\n",
      "['茄', '子', '嫁', '接', '西', '红', '柿', '技', '术']\n",
      "['40', '岁', '男', '人', '肾', '虚', '，', '每', '天', '吃', '点', '，', '补', '充', '雄', '激', '素', '，', '年', '轻', '20', '岁', '！']\n",
      "['外', '星', '人', '疑', '是', '地', '球', '人', '祖', '先', '，', '科', '学', '家', '似', '发', '现', '其', '尸', '体', '，', '构', '造', '和', '人', '类', '多', '处', '相', '似']\n",
      "['华', '为', '手', '机', '进', '货', '价', '曝', '光', '，', '看', '看', '你', '买', '的', '手', '机', '多', '花', '了', '多', '少', '钱', '？']\n",
      "['[UNK]', '要', '解', '散', '?', '王', '源', '已', '成', '立', '个', '人', '工', '作', '室']\n",
      "['李', '天', '一', '获', '超', '快', '减', '刑', '提', '前', '6', '年', '出', '狱', '，', '梦', '鸽', '满', '面', '春', '光']\n",
      "['很', '想', '儿', '女', '双', '全', '，', '想', '知', '道', '怎', '样', '才', '能', '怀', '上', '男', '孩', '？', '如', '何', '生', '儿', '子', '的', '秘', '诀']\n",
      "['一', '把', '八', '角', '，', '专', '治', '前', '列', '腺', '，', '七', '天', '断', '根', '，', '一', '辈', '子', '不', '反', '复']\n",
      "['老', '太', '好', '心', '救', '下', '一', '条', '白', '蛇', '，', '企', '料', '那', '蛇', '却', '咬', '死', '了', '老', '太', '的', '儿', '子']\n",
      "['又', '一', '主', '持', '人', '癌', '症', '复', '发', '！', '脸', '部', '变', '形', '不', '得', '已', '离', '开', '舞', '台', '，', '网', '友', '：', '心', '疼']\n",
      "['清', '朝', '末', '年', '广', '西', '僵', '尸', '袭', '人', '事', '件', '是', '真', '是', '假', '？', '官', '府', '亲', '自', '出', '动', '镇', '压']\n",
      "['内', '科', '医', '生', '推', '荐', ':', '冬', '天', '这', '样', '吃', '，', '3', '个', '月', '治', '好', '15', '年', '的', '老', '胃', '病', '，', '珍', '藏']\n",
      "['王', '者', '荣', '耀', '年', '底', '停', '止', '运', '营', '？', '天', '美', '听', '了', '想', '打', '人', '！']\n",
      "['《', '前', '半', '生', '2', '》', '女', '主', '不', '是', '马', '伊', '琍', '，', '另', '换', '其', '人', '？']\n",
      "['有', '人', '说', '空', '心', '菜', '是', '蔬', '菜', '里', '的', '[UNK]', '毒', '中', '之', '王', '[UNK]', '，', '还', '能', '放', '心', '吃', '吗', '？']\n",
      "['[UNK]', '飞', '机', '就', '要', '起', '飞', '，', '一', '个', '男', '人', '在', '机', '舱', '口', '跪', '下', '！', '[UNK]']\n",
      "['日', '本', '311', '大', '地', '震', '后', '7', '年', '福', '岛', '玫', '瑰', '园', '如', '今', '杂', '草', '丛', '生']\n",
      "['全', '面', '取', '消', '中', '考', '？', '美', '术', '纳', '入', '高', '考', '？', '不', '属', '实', '！']\n",
      "['交', '通', '违', '法', '[UNK]', '清', '零', '[UNK]', '行', '动', '开', '始', '了', '！']\n",
      "['吃', '了', '荔', '枝', '，', '就', '会', '查', '出', '酒', '驾', '？', '交', '警', '的', '实', '验', '告', '诉', '你', '这', '是', '真', '的', '！']\n",
      "['喝', '酒', '不', '醉', '的', '技', '巧', '竟', '是', '这', '？', '绝', '对', '意', '想', '不', '到', '的', '千', '杯', '不', '醉', '小', '妙', '招']\n",
      "['手', '机', '上', '免', '费', '看', '直', '播', '，', '世', '界', '上', '所', '有', '频', '道', '都', '能', '观', '看', '，', '打', '开', '微', '信', '就', '可', '以']\n",
      "['于', '文', '华', '母', '亲', '去', '世', '，', '葬', '礼', '上', '这', '首', '歌', '让', '于', '文', '华', '痛', '哭', '流', '涕', '，', '全', '村', '都', '难', '过', '落', '泪']\n",
      "['好', '尴', '尬', '穿', '中', '国', '风', '百', '蝶', '衣', '亮', '相', '戛', '纳', '红', '毯', '，', '被', '保', '安', '驱', '赶', '了', '三', '次']\n",
      "['九', '江', '市', '民', '疑', '在', '九', '九', '隆', '超', '市', '买', '到', '[UNK]', '塑', '料', '紫', '菜', '[UNK]', '，', '打', '汤', '后', '嚼', '不', '烂', '难', '扯', '断']\n",
      "['腰', '突', '俩', '把', '野', '生', '艾', '草', '轻', '松', '搞', '定', '，', '3', '天', '去', '根', '不', '复', '发', '，', '腰', '腿', '疼', '也', '管', '用']\n",
      "['男', '人', '这', '样', '吃', '鸡', '蛋', '，', '金', '枪', '不', '倒', '半', '小', '时', '，', '吃', '三', '天', '就', '见', '效', '！']\n",
      "['无', '核', '葡', '萄', '用', '避', '孕', '药', '，', '吃', '了', '会', '绝', '育', '？', '！']\n",
      "['农', '村', '人', '注', '意', '了', '，', '这', '30', '个', '省', '农', '村', '户', '口', '要', '取', '消', '了']\n",
      "['小', '米', '[UNK]', '系', '列', '新', '机', '曝', '光', '，', '骁', '龙', '835', '+', '5', '.', '99', '寸', '全', '面', '屏', '2', '.', '0']\n",
      "['央', '视', '公', '布', '17', '种', '[UNK]', '巨', '毒', '食', '品', '[UNK]', '？', '实', '为', '恶', '意', '篡', '改', '编', '造', '和', '旧', '闻', '翻', '炒']\n",
      "['农', '村', '四', '项', '费', '用', '全', '面', '取', '消', '，', '别', '不', '知', '道', '白', '交', '了']\n",
      "['5', '月', '1', '日', '起', '全', '国', '公', '交', '成', '失', '联', '儿', '童', '守', '护', '点', '？', '真', '假', '？']\n",
      "['一', '家', '三', '口', '吃', '[UNK]', '黑', '美', '人', '[UNK]', '西', '瓜', '死', '亡']\n",
      "['史', '上', '最', '难', '找', '的', '方', '子', '，', '为', '了', '家', '人', '留', '着', '吧', '，', '家', '家', '都', '有', '需', '要', '！']\n",
      "['开', '塞', '露', '可', '以', '洗', '脸', '美', '白', '？', '学', '会', '使', '用', '一', '瓶', '开', '塞', '露', '，', '脸', '就', '可', '快', '速', '变', '白']\n",
      "\n",
      "tokens_tensors.shape   = torch.Size([64, 63])\n",
      "tensor([[ 101, 5722, 3300,  ...,    0,    0,    0],\n",
      "        [ 101, 4255, 3160,  ..., 8013,  102,    0],\n",
      "        [ 101,  711, 2506,  ..., 8013,  102,    0],\n",
      "        ...,\n",
      "        [ 101,  671, 2157,  ...,    0,    0,    0],\n",
      "        [ 101, 1380,  677,  ...,    0,    0,    0],\n",
      "        [ 101, 2458, 1853,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([64, 63])\n",
      "tensor([[ 101, 5722, 3300,  ...,    0,    0,    0],\n",
      "        [ 101, 4255, 3160,  ..., 8013,  102,    0],\n",
      "        [ 101,  711, 2506,  ..., 8013,  102,    0],\n",
      "        ...,\n",
      "        [ 101,  671, 2157,  ...,    0,    0,    0],\n",
      "        [ 101, 1380,  677,  ...,    0,    0,    0],\n",
      "        [ 101, 2458, 1853,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([64, 63])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([64])\n",
      "tensor([2, 0, 2, 2, 1, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
      "        2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0,\n",
      "        0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape}\n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "除了label_ids 以外，其他3个tensors的每个样本的最后都为0， 这是因为每个样本的tokens序列基本上长度都会不懂，需要用paddings.\n",
    "![](https://leemeng.tw/images/bert/from_raw_data_to_bert_compatible.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}